{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FP-GAN-v1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOShFGoRRy15MVqqTiRD5om",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mikler87/dbg-pds/blob/master/FP_GAN_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Iw0L44GI4k",
        "colab_type": "text"
      },
      "source": [
        "# Initialisation and authentication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1DcdfNrEeQs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "8cfdd46c-32e5-4188-f713-a8f6f97e2ef6"
      },
      "source": [
        "import os, sys, math\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.stats\n",
        "import time\n",
        "import datetime\n",
        "import string\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "if 'google.colab' in sys.modules: # Colab-only Tensorflow version selector\n",
        "  %tensorflow_version 2.0.0\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "import pickle\n",
        "#AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import sys\n",
        "#import tensorboardcolab\n",
        "#!pip install tensorboard==2.2.2\n",
        "from tensorboard import version\n",
        "\n",
        "print('__Python VERSION:', sys.version)\n",
        "print('__Tensorflow VERSION',tf.__version__)\n",
        "print('__Tensorboard VERSION:', version.VERSION)\n",
        "print('__pyTorch VERSION:', torch.__version__)\n",
        "print('__Torchvision VERSION',torchvision.__version__)\n",
        "print('__CUDA VERSION',torch.version.cuda )\n",
        "from subprocess import call\n",
        "!nvcc --version\n",
        "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
        "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
        "print('__Devices')\n",
        "if torch.cuda.device_count() >0:\n",
        "  print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
        "  print('Current device name',torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  print('No Cuda Device')\n",
        "\n",
        "\n",
        "\n",
        "print('Mounting google drive...')\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd \"/content/drive/My Drive\"\n",
        "!mkdir FinalProject-GCP\n",
        "%cd \"/content/drive/My Drive/FinalProject-GCP\"\n",
        "\n",
        "#  we authenticate with the GCS to enable access to Dataproc and AI-Platform.\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "#%pwd\n",
        "#%ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.0.0`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n",
            "Tensorflow version 2.3.0\n",
            "__Python VERSION: 3.6.9 (default, Jul 17 2020, 12:50:27) \n",
            "[GCC 8.4.0]\n",
            "__Tensorflow VERSION 2.3.0\n",
            "__Tensorboard VERSION: 2.3.0\n",
            "__pyTorch VERSION: 1.6.0+cu101\n",
            "__Torchvision VERSION 0.7.0+cu101\n",
            "__CUDA VERSION 10.1\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "__CUDNN VERSION: 7603\n",
            "__Number CUDA Devices: 1\n",
            "__Devices\n",
            "Active CUDA Device: GPU 0\n",
            "Current device name Tesla P100-PCIE-16GB\n",
            "Mounting google drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive\n",
            "mkdir: cannot create directory ‘FinalProject-GCP’: File exists\n",
            "/content/drive/My Drive/FinalProject-GCP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwLS4w1IGGsb",
        "colab_type": "text"
      },
      "source": [
        "# Project creation GCP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OThNWTmiEn6Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "4f78c190-4728-4454-e9a5-bb4df6ce0906"
      },
      "source": [
        "PROJECT = 'fpcityuni-2020' ### USE YOUR PROJECT ID HERE! ###\n",
        "!gcloud config set project $PROJECT\n",
        "REGION = 'us-central1'\n",
        "CLUSTER = '{}-cluster'.format(PROJECT)\n",
        "!gcloud config set compute/region $REGION\n",
        "!gcloud config set dataproc/region $REGION\n",
        "\n",
        "!gcloud config list # show some information"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Updated property [compute/region].\n",
            "Updated property [dataproc/region].\n",
            "[component_manager]\n",
            "disable_update_check = True\n",
            "[compute]\n",
            "gce_metadata_read_timeout_sec = 0\n",
            "region = us-central1\n",
            "[core]\n",
            "account = 87mazar@gmail.com\n",
            "project = fpcityuni-2020\n",
            "[dataproc]\n",
            "region = us-central1\n",
            "\n",
            "Your active configuration is: [default]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElK0yYTAE4jy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1bdc0ec6-22eb-48dc-9e95-29c81b5d8f55"
      },
      "source": [
        "BUCKET = 'gs://{}-storage'.format(PROJECT)\n",
        "!gsutil mb $BUCKET"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating gs://fpcityuni-2020-storage/...\n",
            "ServiceException: 409 Bucket fpcityuni-2020-storage already exists.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yGop6owGZ9T",
        "colab_type": "text"
      },
      "source": [
        "# Set local directory\n",
        "Make sure the python file and dataset label sheet (txt) is saved there\n",
        "The dataset is saved in the cloud for google drive cannot handle big size folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfNtFV6hE8ND",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3f1b2a1-cac8-430c-cf86-c75deca438b0"
      },
      "source": [
        "%cd \"/content/drive/My Drive/FinalProject-GCP/\"\n",
        "!ls -1"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/FinalProject-GCP\n",
            "AllFile.ipynb\n",
            "brats_auc.py\n",
            "brats_syn_256_lambda0.1\n",
            "celeba_1\n",
            "City_server_FP_GAN.ipynb\n",
            "CodeCombined.py\n",
            "CodeCombined_tf_1x_CPU.py\n",
            "CodeCombined_tf_1x_GPU.py\n",
            "CodeCombined_tf_1x.py\n",
            "CodeCombined_tf_2x_CPU.py\n",
            "CodeCombined_tf_2x_GPU.py\n",
            "'Copy of FP-GAN.ipynb'\n",
            "data\n",
            "dataset__gcp.zip\n",
            "dataset.zip\n",
            "data.zip\n",
            "dist\n",
            "download.sh\n",
            "Fixed-Point-GAN.ipynb\n",
            "fpcityuni-2020-59e3d89958d7.json\n",
            "FP-GAN.ipynb\n",
            "FP-GAN-Org-v0.ipynb\n",
            "FP-GAN-v1.ipynb\n",
            "FP-GAN-v2.ipynb\n",
            "FP-GAN-v3.ipynb\n",
            "FP-GAN-v4.ipynb\n",
            "GAN_MSG.py\n",
            "GAN_nGPU_GCP_Method1_LastResolutionLoss_Separable_conv_additional_layer.py\n",
            "GAN_nGPU_GCP_Method1_LastResolutionLoss_Separable_conv.py\n",
            "GAN_nGPU_GCP_Method1_LastResolutionLoss_v2.py\n",
            "GAN_nGPU_GCP_Method2_scaledimagesaveraging.py\n",
            "GAN_nGPU_GCP_Method2_scaledimagesaveraging_Separable_conv_additional_layer.py\n",
            "GAN_nGPU_GCP_Method2_scaledimagesaveraging_Separable_conv.py\n",
            "GAN_nGPU_GCP_Method2_scaledimagesaveraging_v2.py\n",
            "GAN_nGPU_GCP_Org_1024.py\n",
            "GAN_tf_2x_nGPU__New.py\n",
            "GAN_tf_2x_nGPU.py\n",
            "GAN_tf_nGPU__City_server_no_cloud.py\n",
            "GAN_tf_nGPU__City_server_no_cloud_without_tensorboard.py\n",
            "GAN_tf_nGPU_for_city_servers.py\n",
            "GAN_tf_nGPU_for_GCP_2_test_losses_Method1_LastResolutionLoss.py\n",
            "GAN_tf_nGPU_for_GCP_2_test_losses_Method2_scaledimagesaveraging.py\n",
            "GAN_tf_nGPU_for_GCP_2_test_losses_Method2_singleloss.py\n",
            "GAN_tf_nGPU_for_GCP_2_test_losses.py\n",
            "GAN_tf_nGPU_for_GCP.py\n",
            "initialization-actions-master\n",
            "LICENSE\n",
            "list_attr_celeba_hd.txt\n",
            "list_attr_celeba.txt\n",
            "List_dir.pkl\n",
            "List_filenames.pkl\n",
            "List_OS_SAVED.pkl\n",
            "logger.py\n",
            "main.py\n",
            "model.py\n",
            "OS.py\n",
            "__pycache__\n",
            "README.md\n",
            "requirements1.txt\n",
            "setup1.py\n",
            "setup.py\n",
            "solver.py\n",
            "trainer\n",
            "trainer.egg-info\n",
            "'v3 and v4-full generator Separate conv 512 bottle neck.jpg'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3s49qMvFZUu",
        "colab_type": "text"
      },
      "source": [
        "# Generator Discriminator without discriminative std\n",
        "There are 2 versions of the loss: One where the loss is considered the loss of the last 1024x1024 image while method 2 is the avarage loss of multiscaled images\n",
        "\n",
        "There are 4 differents versions of Generator Discriminator. Each One is saved in different files. Here, we will implement The below design:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p15d68XgQpwL",
        "colab_type": "text"
      },
      "source": [
        "![](https://drive.google.com/uc?id=1KDhSkBfqAOJUTNwOBxyFfZesPhqedLMx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb1gIwp5HQV6",
        "colab_type": "text"
      },
      "source": [
        "#Method 1: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHBCA7xFHmcz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60d77f55-5744-47bb-ece3-ef639c0bf026"
      },
      "source": [
        "%%writefile GAN_tf_nGPU_for_GCP_2_test_losses_Method1_LastResolutionLoss.py\n",
        "\n",
        "# inserting a file with huge number of images doesn't work with google drive, gcp is needed to read the data from there\n",
        "# Model\n",
        "# works with 2 gpu not 3 and with 20 batch size 10.30 mins\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.nn import ModuleList, Conv2d\n",
        "\n",
        "\n",
        "class depthwise_separable_conv(nn.Module):\n",
        "    def __init__(self, nin, nout, kernels_per_layer=1, K_depth=3, S_depth=1, P_depth=1, D_depth=1, K_point=1, S_point=1, P_point=0, D_point=1, bias=False):\n",
        "        super(depthwise_separable_conv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(nin, nin * kernels_per_layer, kernel_size=K_depth,stride=S_depth, padding=P_depth,dilation=D_depth, groups=nin,bias=bias)\n",
        "        self.pointwise = nn.Conv2d(nin * kernels_per_layer, nout, kernel_size=K_point,stride=S_point,padding=P_point,dilation=D_point, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.pointwise(out)\n",
        "        return out\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual Block with instance normalization.\"\"\"\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            depthwise_separable_conv(nin=dim_in,nout=dim_out, kernels_per_layer=1,K_depth=3,S_depth=1,P_depth=1,D_depth=1,K_point=1,S_point=1,P_point=0,D_point=1,bias=False),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            depthwise_separable_conv(nin=dim_in,nout=dim_out, kernels_per_layer=1,K_depth=3,S_depth=1,P_depth=1,D_depth=1,K_point=1,S_point=1,P_point=0,D_point=1,bias=False),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.main(x)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"Generator network.\"\"\"\n",
        "    def __init__(self, ngpu=0, conv_dim=16, c_dim=1, repeat_num=8,depth=5):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = torch.cuda.device_count()\n",
        "        \n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(3+c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False))\n",
        "        layers.append(nn.InstanceNorm2d(conv_dim, affine=True, track_running_stats=True))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        # Down-sampling layers.\n",
        "        curr_dim = conv_dim\n",
        "        for i in range(depth):\n",
        "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False))\n",
        "            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True, track_running_stats=True))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        # Bottleneck layers.\n",
        "        for i in range(repeat_num):\n",
        "            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))\n",
        "        \n",
        "        self.main_DownSampling_bottleneck = nn.Sequential(*layers)\n",
        "        \n",
        "        # create a module list of the other required general convolution blocks\n",
        "        self.UpsamplingLayers = ModuleList()\n",
        "        self.rgb_converters = ModuleList()\n",
        "\n",
        "        # Up-sampling layers.\n",
        "        def conv_block(curr_dim,K=4,S=2,P=1,bias_=False):\n",
        "          return nn.Sequential( nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=K, stride=S, padding=P, bias=bias_),\n",
        "            nn.InstanceNorm2d(curr_dim//2, affine=True, track_running_stats=True),\n",
        "            nn.ReLU(inplace=True))\n",
        "        \n",
        "        # to rgb conversion\n",
        "        def to_rgb(in_channels):\n",
        "          layers_rgb = []\n",
        "          layers_rgb.append(nn.Sequential(Conv2d(in_channels, 3, (1, 1),(1,1),(0,0), bias=True)))\n",
        "          #layers_rgb.append(nn.Tanh())\n",
        "          out = nn.Sequential(*layers_rgb)\n",
        "          return out\n",
        "\n",
        "        for i in range(depth):\n",
        "          if i != (depth-1):\n",
        "            self.UpsamplingLayers.append(conv_block(curr_dim,4,2,1,False))\n",
        "            rgb = to_rgb(int(curr_dim//2))\n",
        "            self.rgb_converters.append(rgb)\n",
        "            curr_dim = curr_dim // 2\n",
        "          else :\n",
        "            self.UpsamplingLayers.append(conv_block(curr_dim,4,2,1,False)) # curr_dim,7,1,3,False)\n",
        "            rgb = to_rgb(int(curr_dim//2))\n",
        "            self.rgb_converters.append(rgb)\n",
        "            curr_dim = curr_dim // 2\n",
        "          \n",
        "\n",
        "    def forward(self, x, c):\n",
        "        # Replicate spatially and concatenate domain information.\n",
        "        c = c.view(c.size(0), c.size(1), 1, 1)\n",
        "        c = c.repeat(1, 1, x.size(2), x.size(3))\n",
        "        x = torch.cat([x, c], dim=1)\n",
        "\n",
        "        #print(self.UpsamplingLayers)\n",
        "        #print(self.rgb_converters)\n",
        "\n",
        "        outputs=[]\n",
        "        Y=self.main_DownSampling_bottleneck(x) # start the computational pipeline\n",
        "        #counter=0\n",
        "        #print(\"Y after downsmapling and bottleneck\",Y.size())\n",
        "        for block, converter in zip(self.UpsamplingLayers, self.rgb_converters):\n",
        "            #print(\"Y again\",Y.size())\n",
        "            Y = block(Y)\n",
        "            outputs.append(converter(Y))\n",
        "            #counter=counter+1\n",
        "            #print(counter)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "      \n",
        "    @staticmethod\n",
        "    def adjust_dynamic_range(data, drange_in=(-1, 1), drange_out=(0, 1)):\n",
        "      \"\"\"\n",
        "      adjust the dynamic colour range of the given input data\n",
        "      :param data: input image data\n",
        "      :param drange_in: original range of input\n",
        "      :param drange_out: required range of output\n",
        "      :return: img => colour range adjusted images\n",
        "      \"\"\"\n",
        "      if drange_in != drange_out:\n",
        "        scale = (np.float32(drange_out[1]) - np.float32(drange_out[0])) / (np.float32(drange_in[1]) - np.float32(drange_in[0]))\n",
        "        bias = (np.float32(drange_out[0]) - np.float32(drange_in[0]) * scale)\n",
        "        data = data * scale + bias\n",
        "      return torch.clamp(data, min=0, max=1)\n",
        "\n",
        "\n",
        "\n",
        "# Discriminator code using blocks\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.nn import ModuleList, Conv2d\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"Discriminator network with PatchGAN.\"\"\"\n",
        "    def __init__(self, ngpu=0, image_size=1024, conv_dim=16, c_dim=5, repeat_num=8,depth=5):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu=torch.cuda.device_count()\n",
        "\n",
        "        def from_rgb(out_channels):\n",
        "          return Conv2d(3, out_channels, (1, 1), bias=True)\n",
        "\n",
        "        curr_dim = conv_dim\n",
        "        self.rgb_to_features = ModuleList()\n",
        "\n",
        "  \n",
        "        # Discrimintor Conv Block\n",
        "        def dis_initial_conv_block(curr_dim,K=4,S=2,P=1,bias_=False):\n",
        "          return nn.Sequential(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=K, stride=S, padding=P,bias=False),\n",
        "                               nn.LeakyReLU(0.01))\n",
        "\n",
        "\n",
        "        def dis_mid_conv_block(curr_dim,K=4,S=2,P=1,bias_=False):\n",
        "          return nn.Sequential(nn.Conv2d(curr_dim+3, curr_dim, kernel_size=3, stride=1, padding=1,bias=False), nn.LeakyReLU(0.01),\n",
        "                               nn.Conv2d(curr_dim, curr_dim*2, kernel_size=K, stride=S, padding=P,bias=False),\n",
        "                               nn.LeakyReLU(0.01))\n",
        "        \n",
        "        #def dis_mid_conv_block(curr_dim,K=4,S=2,P=1,bias_=False):\n",
        "          #return nn.Sequential(nn.Conv2d(curr_dim+3, curr_dim*2, kernel_size=K, stride=S, padding=P,bias=False),\n",
        "                               #nn.LeakyReLU(0.01))\n",
        "          \n",
        "        def dis_conv_block_without_rgb(curr_dim,K=4,S=2,P=1,bias_=False):\n",
        "          return nn.Sequential(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=K, stride=S, padding=P,bias=False),\n",
        "                               nn.LeakyReLU(0.01))\n",
        "\n",
        "        # Initiate discrimator blocks module list\n",
        "        self.Dis_blocks = ModuleList()\n",
        "\n",
        "        # Fill the module list of both discriminator blocks and rgb converter\n",
        "        for i in range(1, repeat_num):\n",
        "          if i == 1:\n",
        "            self.Dis_blocks.append(dis_initial_conv_block(curr_dim,4,2,1,False))\n",
        "            from_rgb_ = from_rgb(int(curr_dim))\n",
        "            self.rgb_to_features.append(from_rgb_)            \n",
        "            curr_dim = curr_dim * 2\n",
        "          elif i < (repeat_num-2) and (i != 1):\n",
        "            self.Dis_blocks.append(dis_mid_conv_block(curr_dim,4,2,1,False))\n",
        "            from_rgb_ = from_rgb(int(curr_dim))\n",
        "            self.rgb_to_features.append(from_rgb_)            \n",
        "            curr_dim = curr_dim * 2\n",
        "          elif i == repeat_num:\n",
        "            self.Dis_blocks.append(dis_conv_block_without_rgb(curr_dim,4,2,1,False))\n",
        "          else:\n",
        "            self.Dis_blocks.append(dis_conv_block_without_rgb(curr_dim,4,2,1,False))      \n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        kernel_size = int(image_size / np.power(2, repeat_num)) # this code was modified to fit kernel 8x8 to get the class\n",
        "        print(\"kernel size\",kernel_size)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        if image_size == 1024:\n",
        "          self.conv2=nn.Sequential(nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, stride=1, padding=0,bias=False),\n",
        "                                   nn.LeakyReLU(0.01),nn.Conv2d(c_dim, c_dim, kernel_size=kernel_size-1, stride=1, padding=0,bias=False),\n",
        "                                   nn.LeakyReLU(0.01),nn.Conv2d(c_dim, c_dim, kernel_size=kernel_size-1, stride=1, padding=0,bias=False), nn.LeakyReLU(0.01))\n",
        "        elif image_size == 512:\n",
        "          self.conv2=nn.Sequential(nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size+2, stride=1, padding=0,bias=False),\n",
        "                                   nn.LeakyReLU(0.01)) # doesn't work with 512\n",
        "        \n",
        "        elif image_size == 256:\n",
        "          self.conv2=nn.Sequential(nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size+1, stride=1, padding=0,bias=False),\n",
        "                                   nn.LeakyReLU(0.01)) # doesn't work with 256\n",
        "\n",
        "        \n",
        "        \n",
        "    def forward(self, inputs):\n",
        "\n",
        "        # First Input\n",
        "        y = self.rgb_to_features[0](inputs[0])\n",
        "        #print(\"y rgb to features\",y.size())\n",
        "        y = self.Dis_blocks[0](y)\n",
        "        #print(\"dis_block\",y.size())\n",
        "\n",
        "        #counter=0\n",
        "          \n",
        "        for x, block in \\\n",
        "                zip((inputs[1:]),\n",
        "                    (self.Dis_blocks[1:])):\n",
        "            \n",
        "            #def get_shapes(Depth):\n",
        "            #  return [inputs[i].size() for i in range(Depth)]\n",
        "            #print('full input shape:', get_shapes(5) )\n",
        "\n",
        "            #print('x: ', x.size())\n",
        "            #input_part = converter(x)  # convert the input:\n",
        "            #print(\"input_part: \", input_part.size() )\n",
        "            #print(converter)\n",
        "            y = torch.cat((x, y), dim=1)  # concatenate the inputs:\n",
        "            #print(\"concat input_part(here x instead) and y:\",y.size())\n",
        "            #print('block:', block)\n",
        "            y = block(y)  # apply the block\n",
        "            #print(\"y block(y):\",y.size())\n",
        "            #counter=counter+1\n",
        "            #print(counter)\n",
        "\n",
        "        y = self.Dis_blocks[-2](y)\n",
        "        #print(y.size())\n",
        "        y = self.Dis_blocks[-1](y)\n",
        "        #print(\"y last\",y.size())\n",
        "        # calculate the final block:\n",
        "        #input_part = self.final_converter(inputs[0])\n",
        "        #y = th.cat((input_part, y), dim=1)\n",
        "        #y = self.final_block(y)\n",
        "\n",
        "        out_src = self.conv1(y)\n",
        "        #print(\"out_src: \",out_src.size())\n",
        "        out_cls = self.conv2(y)\n",
        "        #print(\"out_cls: \",out_cls.size())\n",
        "        return out_src, out_cls.view(out_cls.size(0), out_cls.size(1))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\"\"\"Residual Block with instance normalization.\"\"\"\"\"\"\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True),\n",
        "            nn.ReLU(inplace=True), # nn.ReLU(inplace=True) originally\n",
        "            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.main(x)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\"\"\"Generator network.\"\"\"\"\"\"\n",
        "    def __init__(self, ngpu=0, conv_dim=64, c_dim=5, repeat_num=6):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = torch.cuda.device_count()\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(3+c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False))\n",
        "        layers.append(nn.InstanceNorm2d(conv_dim, affine=True, track_running_stats=True))\n",
        "        layers.append(nn.ReLU(inplace=True)) # before it was True\n",
        "\n",
        "        # Down-sampling layers.\n",
        "        curr_dim = conv_dim\n",
        "        for i in range(2):\n",
        "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False))\n",
        "            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True, track_running_stats=True))\n",
        "            layers.append(nn.ReLU(inplace=True)) # before it was True\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        # Bottleneck layers.\n",
        "        for i in range(repeat_num):\n",
        "            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))\n",
        "\n",
        "        # Up-sampling layers.\n",
        "        for i in range(2):\n",
        "            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=4, stride=2, padding=1, bias=False))\n",
        "            layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True, track_running_stats=True))\n",
        "            layers.append(nn.ReLU(inplace=True)) # was True\n",
        "            curr_dim = curr_dim // 2\n",
        "\n",
        "        layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False))\n",
        "        self.main = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        # Replicate spatially and concatenate domain information.\n",
        "        c = c.view(c.size(0), c.size(1), 1, 1)\n",
        "        c = c.repeat(1, 1, x.size(2), x.size(3))\n",
        "        x = torch.cat([x, c], dim=1)\n",
        "        return self.main(x)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\"\"\"Discriminator network with PatchGAN.\"\"\"\"\"\"\n",
        "    def __init__(self, ngpu=0, image_size=128, conv_dim=64, c_dim=5, repeat_num=6):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu=torch.cuda.device_count()\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(3, conv_dim, kernel_size=4, stride=2, padding=1))\n",
        "        layers.append(nn.LeakyReLU(0.01))\n",
        "         \n",
        "\n",
        "        curr_dim = conv_dim\n",
        "        for i in range(1, repeat_num):\n",
        "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1))\n",
        "            layers.append(nn.LeakyReLU(0.01))\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        kernel_size = int(image_size / np.power(2, repeat_num))\n",
        "        self.main = nn.Sequential(*layers)\n",
        "        self.conv1 = nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, bias=False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = self.main(x)\n",
        "        out_src = self.conv1(h)\n",
        "        out_cls = self.conv2(h)\n",
        "        # h1 clone removed..probably not needed\n",
        "        return out_src, out_cls.view(out_cls.size(0), out_cls.size(1))\n",
        "\"\"\"\n",
        "\n",
        "# logger\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    \"\"\"Tensorboard logger.\"\"\"\n",
        "\n",
        "    def __init__(self, log_dir):\n",
        "        \"\"\"Initialize summary writer.\"\"\"\n",
        "        # self.writer = tf.summary.FileWriter(log_dir) # depreciated\n",
        "        # self.writer = tf.compat.v1.summary.FileWriter(log_dir) # new working one\n",
        "        \n",
        "        #self.writer = tf.contrib.summary.FileWriter(log_dir)\n",
        "        #self.writer = tf.train.SummaryWriter(log_dir)\n",
        "        \n",
        "        # for version 2.xx\n",
        "        self.writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "    def scalar_summary(self, tag, value, step):\n",
        "        \"\"\"Add scalar summary.\"\"\"\n",
        "        #summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
        "        #self.writer.add_summary(summary, step)\n",
        "        \n",
        "        # tf version 2.x\n",
        "        with self.writer.as_default():\n",
        "            tf.summary.scalar(tag, value, step=step)\n",
        "            self.writer.flush()\n",
        "\n",
        "            \n",
        "# brats_auc\n",
        "\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "imgs = glob(\"brats_syn_256_lambda0.1/results/*.jpg\")\n",
        "gts = [0]*975 + [1]*486\n",
        "\n",
        "preds = []\n",
        "\n",
        "for i in range(len(imgs)):\n",
        "    im = np.array(Image.open(\"brats_syn_256_lambda0.1/results/{}-images.jpg\".format(i+1)))\n",
        "    rows = np.split(im, im.shape[0]//256, axis=0)\n",
        "\n",
        "    for r in rows:\n",
        "        cols = np.split(r, 5, axis=1)\n",
        "        preds.append(np.max( np.mean(cols[1], axis=-1) ))\n",
        "\n",
        "#print(roc_auc_score(gts, preds))\n",
        "\n",
        "            \n",
        "# Solver\n",
        "\n",
        "#from model import Generator\n",
        "#from model import Discriminator\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "from sklearn.cluster import KMeans\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch.nn.parallel import DataParallel\n",
        "\n",
        "\n",
        "class Solver(object):\n",
        "    \"\"\"Solver for training and testing Fixed-Point GAN.\"\"\"\n",
        "\n",
        "    def __init__(self, data_loader, config):\n",
        "        \"\"\"Initialize configurations.\"\"\"\n",
        "\n",
        "        # Data loader.\n",
        "        self.data_loader = data_loader\n",
        "\n",
        "        # Model configurations.\n",
        "        self.c_dim = config.c_dim\n",
        "        self.c2_dim = config.c2_dim\n",
        "        self.image_size = config.image_size\n",
        "        self.g_conv_dim = config.g_conv_dim\n",
        "        self.d_conv_dim = config.d_conv_dim\n",
        "        self.g_repeat_num = config.g_repeat_num\n",
        "        self.d_repeat_num = config.d_repeat_num\n",
        "        self.lambda_cls = config.lambda_cls\n",
        "        self.lambda_rec = config.lambda_rec\n",
        "        self.lambda_gp = config.lambda_gp\n",
        "        self.lambda_id = config.lambda_id\n",
        "        self.depth = config.depth\n",
        "\n",
        "        # Training configurations.\n",
        "        self.dataset = config.dataset\n",
        "        self.batch_size = config.batch_size\n",
        "        self.num_iters = config.num_iters\n",
        "        self.num_iters_decay = config.num_iters_decay\n",
        "        self.g_lr = config.g_lr\n",
        "        self.d_lr = config.d_lr\n",
        "        self.n_critic = config.n_critic\n",
        "        self.beta1 = config.beta1\n",
        "        self.beta2 = config.beta2\n",
        "        self.resume_iters = config.resume_iters\n",
        "        self.selected_attrs = config.selected_attrs\n",
        "\n",
        "        # Test configurations.\n",
        "        self.test_iters = config.test_iters\n",
        "\n",
        "        # Miscellaneous.\n",
        "        self.use_tensorboard = config.use_tensorboard\n",
        "        #self.ngpu= config.ngpu\n",
        "        self.ngpu= torch.cuda.device_count()\n",
        "        self.device = torch.device('cuda' if (torch.cuda.is_available() and self.ngpu>0) else 'cpu')\n",
        "        print('Number of Cuda devices: ',torch.cuda.device_count())\n",
        "\n",
        "        # Directories.\n",
        "        self.log_dir = config.log_dir\n",
        "        self.sample_dir = config.sample_dir\n",
        "        self.model_save_dir = config.model_save_dir\n",
        "        self.result_dir = config.result_dir\n",
        "\n",
        "        # Step size.\n",
        "        self.log_step = config.log_step\n",
        "        self.sample_step = config.sample_step\n",
        "        self.model_save_step = config.model_save_step\n",
        "        self.lr_update_step = config.lr_update_step\n",
        "\n",
        "        self.rank=0\n",
        "        self.world_size=1\n",
        "\n",
        "        # Build the model and tensorboard.\n",
        "        self.build_model()\n",
        "        if self.use_tensorboard:\n",
        "          self.build_tensorboard()\n",
        "    \n",
        "    def setup(self,rank,world_size):\n",
        "        os.environ['MASTER_ADDR'] = 'localhost'\n",
        "        os.environ['MASTER_PORT'] = '12355'\n",
        "        # initialize the process group\n",
        "        dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Create a generator and a discriminator.\"\"\"\n",
        "\n",
        "        if self.dataset in ['CelebA', 'BRATS', 'Directory']:\n",
        "            self.G = Generator(self.ngpu,self.g_conv_dim, self.c_dim, self.g_repeat_num,self.depth)\n",
        "            self.D = Discriminator(self.ngpu,self.image_size, self.d_conv_dim, self.c_dim, self.d_repeat_num,self.depth) \n",
        "\n",
        "        #self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.g_lr, [self.beta1, self.beta2])\n",
        "        #self.d_optimizer = torch.optim.Adam(self.D.parameters(), self.d_lr, [self.beta1, self.beta2])\n",
        "        self.print_network(self.G, 'Generator')\n",
        "        self.print_network(self.D, 'Discriminator')\n",
        "    \n",
        "        self.G.to(self.device)\n",
        "        self.D.to(self.device)\n",
        "\n",
        "        # Handle multi-gpu if desired\n",
        "        if (self.device.type == 'cuda') and (self.ngpu > 1):\n",
        "          # added for parallelism\n",
        "          #for i in list(range(self.ngpu)):\n",
        "          #  torch.cuda.set_device(i)       \n",
        "          #  self.setup(self.rank,self.world_size)\n",
        "          #  self.G = DistributedDataParallel(self.G, device_ids=[i],output_device=i)\n",
        "          #  self.D = DistributedDataParallel(self.D, device_ids=[i],output_device=i)\n",
        "\n",
        "          #self.setup(self.rank,self.world_size)\n",
        "          #self.G = DistributedDataParallel(self.G, device_ids=list(range(self.ngpu)))\n",
        "          #self.D = DistributedDataParallel(self.D, device_ids=list(range(self.ngpu)))\n",
        "          \n",
        "          self.G = DataParallel(self.G, device_ids=list(range(self.ngpu)))\n",
        "          self.D = DataParallel(self.D, device_ids=list(range(self.ngpu)))\n",
        "\n",
        "          self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.g_lr, [self.beta1, self.beta2])\n",
        "          self.d_optimizer = torch.optim.Adam(self.D.parameters(), self.d_lr, [self.beta1, self.beta2])\n",
        "\n",
        "        else:\n",
        "\n",
        "          self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.g_lr, [self.beta1, self.beta2])\n",
        "          self.d_optimizer = torch.optim.Adam(self.D.parameters(), self.d_lr, [self.beta1, self.beta2])\n",
        "\n",
        "    def recreate_image(self, codebook, labels, w, h):\n",
        "        \"\"\"Recreate the (compressed) image from the code book & labels\"\"\"\n",
        "        d = codebook.shape[1]\n",
        "        image = np.zeros((w, h, d))\n",
        "        label_idx = 0\n",
        "        for i in range(w):\n",
        "            for j in range(h):\n",
        "                image[i][j] = codebook[labels[label_idx]]\n",
        "                # orginal\n",
        "                # label_idx += 1\n",
        "                label_idx = label_idx + 1\n",
        "        return image\n",
        "\n",
        "    def print_network(self, model, name):\n",
        "        \"\"\"Print out the network information.\"\"\"\n",
        "        num_params = 0\n",
        "        for p in model.parameters():\n",
        "            # original\n",
        "            # num_params += p.numel()\n",
        "            num_params = num_params + p.numel()\n",
        "        print(model)\n",
        "        print(name)\n",
        "        print(\"The number of parameters: {}\".format(num_params))\n",
        "\n",
        "    def restore_model(self, resume_iters):\n",
        "        \"\"\"Restore the trained generator and discriminator.\"\"\"\n",
        "        print('Loading the trained models from step {}...'.format(resume_iters))\n",
        "        G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(resume_iters))\n",
        "        D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(resume_iters))\n",
        "        self.G.load_state_dict(torch.load(G_path, map_location=lambda storage, loc: storage)) # Load all tensors onto the CPU, using a function\n",
        "        self.D.load_state_dict(torch.load(D_path, map_location=lambda storage, loc: storage)) # Load all tensors onto the CPU, using a function # to avoid GPU RAM surge when loading a model checkpoint.\n",
        "\n",
        "    def build_tensorboard(self):\n",
        "        \"\"\"Build a tensorboard logger.\"\"\"\n",
        "        #from logger import Logger\n",
        "        self.logger = Logger(self.log_dir)\n",
        "\n",
        "    def update_lr(self, g_lr, d_lr):\n",
        "        \"\"\"Decay learning rates of the generator and discriminator.\"\"\"\n",
        "        for param_group in self.g_optimizer.param_groups:\n",
        "            param_group['lr'] = g_lr\n",
        "        for param_group in self.d_optimizer.param_groups:\n",
        "            param_group['lr'] = d_lr\n",
        "\n",
        "    def reset_grad(self):\n",
        "        \"\"\"Reset the gradient buffers.\"\"\"\n",
        "        self.g_optimizer.zero_grad()\n",
        "        self.d_optimizer.zero_grad()\n",
        "\n",
        "    def denorm(self, x):\n",
        "        \"\"\"Convert the range from [-1, 1] to [0, 1].\"\"\"\n",
        "        out = (x + 1) / 2\n",
        "        return out.clamp_(0, 1)\n",
        "\n",
        "    def gradient_penalty(self, y, x):\n",
        "        \"\"\"Compute gradient penalty: (L2_norm(dy/dx) - 1)**2.\"\"\"\n",
        "        weight = torch.ones(y.size()).to(self.device)\n",
        "        dydx = torch.autograd.grad(outputs=y,\n",
        "                                   inputs=x,\n",
        "                                   grad_outputs=weight,\n",
        "                                   retain_graph=True,\n",
        "                                   create_graph=True,\n",
        "                                   only_inputs=True)[0]\n",
        "\n",
        "        dydx = dydx.view(dydx.size(0), -1)\n",
        "        dydx_l2norm = torch.sqrt(torch.sum(dydx**2, dim=1))\n",
        "        return torch.mean((dydx_l2norm-1)**2)\n",
        "\n",
        "    def label2onehot(self, labels, dim):\n",
        "        \"\"\"Convert label indices to one-hot vectors.\"\"\"\n",
        "        batch_size = labels.size(0)\n",
        "        out = torch.zeros(batch_size, dim)\n",
        "        out[np.arange(batch_size), labels.long()] = 1\n",
        "        return out\n",
        "\n",
        "    def create_labels(self, c_org, c_dim=5, dataset='CelebA', selected_attrs=None):\n",
        "        \"\"\"Generate target domain labels for debugging and testing.\"\"\"\n",
        "        # Get hair color indices.\n",
        "        if dataset in ['CelebA']:\n",
        "            hair_color_indices = []\n",
        "            for i, attr_name in enumerate(selected_attrs):\n",
        "                if attr_name in ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair']:\n",
        "                    hair_color_indices.append(i)\n",
        "\n",
        "        c_trg_list = []\n",
        "        for i in range(c_dim):\n",
        "            if dataset in ['CelebA']:\n",
        "                c_trg = c_org.clone()\n",
        "                if i in hair_color_indices:  # Set one hair color to 1 and the rest to 0.\n",
        "                    c_trg[:, i] = 1\n",
        "                    for j in hair_color_indices:\n",
        "                        if j != i:\n",
        "                            c_trg[:, j] = 0\n",
        "                else:\n",
        "                    c_trg[:, i] = (c_trg[:, i] == 0)  # Reverse attribute value.\n",
        "            elif dataset == 'BRATS':\n",
        "                c_trg = c_org.clone()\n",
        "                c_trg[:, i] = (c_trg[:, i] == 0)  # Reverse attribute value.\n",
        "            elif dataset == 'Directory':\n",
        "                c_trg = self.label2onehot(torch.ones(c_org.size(0))*i, c_dim)\n",
        "\n",
        "            c_trg_list.append(c_trg.to(self.device))\n",
        "        return c_trg_list\n",
        "\n",
        "    def classification_loss(self, logit, target, dataset='CelebA'):\n",
        "        \"\"\"Compute binary or softmax cross entropy loss.\"\"\"\n",
        "        if dataset in ['CelebA', 'BRATS']:\n",
        "            return F.binary_cross_entropy_with_logits(logit, target, reduction='sum') / logit.size(0)  # size_average=False inside binary_cross_entropy_with_logits\n",
        "        elif dataset == 'Directory':\n",
        "            return F.cross_entropy(logit, target)\n",
        "\n",
        "    def create_grid(self, samples, img_files):\n",
        "        \"\"\"\n",
        "        utility function to create a grid of GAN samples\n",
        "        :param samples: generated samples for storing list[Tensors]\n",
        "        :param img_files: list of names of files to write\n",
        "        :return: None (saves multiple files)\n",
        "        \"\"\"\n",
        "        from torchvision.utils import save_image\n",
        "        from torch.nn.functional import interpolate\n",
        "        from numpy import sqrt, power\n",
        "\n",
        "        # dynamically adjust the colour of the images\n",
        "        samples = [Generator.adjust_dynamic_range(sample) for sample in samples]\n",
        "\n",
        "        # resize the samples to have same resolution:\n",
        "        depth__=5 # quick fix\n",
        "        for i in range(len(samples)):\n",
        "            samples[i] = interpolate(samples[i],\n",
        "                                     scale_factor=power(2,\n",
        "                                                        depth__ - 1 - i))\n",
        "        # save the images:\n",
        "        for sample, img_file in zip(samples, img_files):\n",
        "            save_image(sample, img_file, nrow=int(sqrt(sample.shape[0])),\n",
        "                       normalize=True, scale_each=True, padding=0) \n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train Fixed-Point GAN within a single dataset.\"\"\"\n",
        "        # Set data loader.\n",
        "        if self.dataset in ['CelebA', 'BRATS', 'Directory']:\n",
        "            data_loader = self.data_loader\n",
        "\n",
        "        # Fetch fixed inputs for debugging.\n",
        "        data_iter = iter(data_loader)\n",
        "        x_fixed, c_org = next(data_iter)\n",
        "        x_fixed = x_fixed.to(self.device)\n",
        "        c_fixed_list = self.create_labels(c_org, self.c_dim, self.dataset, self.selected_attrs)\n",
        "\n",
        "        # Learning rate cache for decaying.\n",
        "        g_lr = self.g_lr\n",
        "        d_lr = self.d_lr\n",
        "\n",
        "        # Start training from scratch or resume training.\n",
        "        start_iters = 0\n",
        "        if self.resume_iters:\n",
        "            start_iters = self.resume_iters\n",
        "            self.restore_model(self.resume_iters)\n",
        "\n",
        "        # Start training.\n",
        "        print('Start training...')\n",
        "        start_time = time.time()\n",
        "        for i in range(start_iters, self.num_iters):\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                             1. Preprocess input data                                #\n",
        "            # =================================================================================== #\n",
        "            \n",
        "\n",
        "            # Fetch real images and labels.\n",
        "            try:\n",
        "                x_real, label_org = next(data_iter)\n",
        "                #print(\"x_real\",x_real.size()) # added to test the dimensions\n",
        "                #print(\"label_org\",label_org.size()) # added to test the dimensions\n",
        "            except:\n",
        "                data_iter = iter(data_loader)\n",
        "                x_real, label_org = next(data_iter)\n",
        "\n",
        "                #print(\"x_real: \",x_real.size()) # added to test the dimensions\n",
        "                #print(\"label_org: \",label_org.size()) # added to test the dimensions\n",
        "\n",
        "            # Generate target domain labels randomly.\n",
        "            rand_idx = torch.randperm(label_org.size(0))\n",
        "            label_trg = label_org[rand_idx]\n",
        "\n",
        "            #print(\"rand_idx\",rand_idx.size()) # added to test the dimensions\n",
        "            #print(\"label_trg\",label_trg.size()) # added to test the dimensions\n",
        "\n",
        "            if self.dataset in ['CelebA', 'BRATS']:\n",
        "                c_org = label_org.clone()\n",
        "                c_trg = label_trg.clone()\n",
        "            elif self.dataset == 'Directory':\n",
        "                c_org = self.label2onehot(label_org, self.c_dim)\n",
        "                c_trg = self.label2onehot(label_trg, self.c_dim)\n",
        "            \n",
        "            # added to output everything on 1 gpu \n",
        "            if (self.device.type == 'cuda') and (self.ngpu > 1):\n",
        "              torch.cuda.set_device(0)   \n",
        "            \n",
        "            x_real = x_real.to(self.device)           # Input images.\n",
        "            c_org = c_org.to(self.device)             # Original domain labels.\n",
        "            c_trg = c_trg.to(self.device)             # Target domain labels.\n",
        "            label_org = label_org.to(self.device)     # Labels for computing classification loss.\n",
        "            label_trg = label_trg.to(self.device)     # Labels for computing classification loss.\n",
        "\n",
        "            #print(\"c_org\",c_org.size()) # added to test the dimensions\n",
        "            #print(\"c_targ\",c_trg.size()) # added to test the dimensions\n",
        "            \n",
        "\n",
        "            # testing batch_size and image pyramid\n",
        "            from torch.nn.functional import avg_pool2d\n",
        "            batch_images=x_real  \n",
        "            depth__=5\n",
        "            batch_images = [batch_images] + [avg_pool2d(batch_images, int(np.power(2, i)))\n",
        "                                     for i in range(1, depth__)]\n",
        "                            \n",
        "            #print('image length list: ',len(batch_images)) # added : equal number of depth or layers\n",
        "            #def get_shapes(Depth):\n",
        "            #  return [batch_images[i].size() for i in range(Depth)]\n",
        "            #print('images shape:', get_shapes(depth__) )\n",
        "\n",
        "\n",
        "            # list needs to be added to a torch to be able to be used with to(self.device)\n",
        "            #x_real=[_x_real.to(self.device) for _x_real in batch_images]    # input images\n",
        "            #data = [_data.cuda() for _data in data] \n",
        "            #c_org = c_org.repeat(5,1,1).to(self.device)           # Original domain labels.\n",
        "            #c_trg = c_trg.repeat(5,1,1).to(self.device)         # Target domain labels.\n",
        "            #label_org = label_org.repeat(5,1,1).to(self.device)     # Labels for computing classification loss.\n",
        "            #label_trg = label_trg.repeat(5,1,1).to(self.device)     # Labels for computing classification loss.\n",
        "\n",
        "            #for _x_real in batch_images: \n",
        "            \"\"\"\n",
        "            def padding(ListofTensorImages):\n",
        "              out=[]\n",
        "              for img in ListofTensorImages:\n",
        "                print(img.size())\n",
        "                equ= (1024 - img.size(-1))/2\n",
        "                print(equ)\n",
        "                m = nn.ZeroPad2d(int(equ))\n",
        "                out.append(m(img))\n",
        "              return out\n",
        "\n",
        "            ImagesWithPadding=padding(batch_images)\n",
        "            for i in range(len(ImagesWithPadding)):\n",
        "              print(ImagesWithPadding[i].size())\n",
        "\n",
        "            stackedImages= torch.stack([ImagesWithPadding[0],ImagesWithPadding[1],ImagesWithPadding[2],ImagesWithPadding[3],ImagesWithPadding[4]],dim=0)\n",
        "            \"\"\"\n",
        "\n",
        "            x_real_d=batch_images #.to(self.device)\n",
        "            c_org = c_org.to(self.device)           # Original domain labels.\n",
        "            c_trg = c_trg.to(self.device)         # Target domain labels.\n",
        "            label_org = label_org.to(self.device)     # Labels for computing classification loss.\n",
        "            label_trg = label_trg.to(self.device)     # Labels for computing classification loss.\n",
        "\n",
        "            #def get_shapes_01(Depth):\n",
        "            #  return [batch_images[i].size() for i in range(Depth)],[c_org[i].size() for i in range(Depth)],[c_trg[i].size() for i in range(Depth)],[label_org[i].size() for i in range(Depth)],[label_trg[i].size() for i in range(Depth)]\n",
        "            #print('batch_images, c_org, c_trig , label_org , label_trg :', get_shapes_01(depth__) )  \n",
        "\n",
        "   \n",
        "            # =================================================================================== #\n",
        "            #                             2. Train the discriminator                              #\n",
        "            # =================================================================================== #\n",
        "\n",
        "\n",
        "          #  # Compute loss with real images.\n",
        "            \n",
        "            t0=time.time()\n",
        "            out_src, out_cls = self.D(x_real_d)            \n",
        "            d_loss_real = - torch.mean(out_src)\n",
        "            d_loss_cls = self.classification_loss(out_cls, label_org, self.dataset)\n",
        "            #print(\"Loss with real images\",time.time()-t0)\n",
        "\n",
        "            #print(\"out_src\",out_src.size()) # added to test the dimensions\n",
        "            #print(\"out_cls\",out_cls.size()) # added to test the dimensions\n",
        "            #print(\"d_loss_real\",d_loss_real.size()) # added to test the dimensions\n",
        "            #print(\"d_loss_cls\",d_loss_cls.size()) # added to test the dimensions\n",
        "          \n",
        "          # Compute loss with fake images.\n",
        "            t0=time.time()\n",
        "            delta = self.G(x_real, c_trg) # c_trg is target class  # output 5 images : output the difference   \n",
        "            \n",
        "            #def get_shapes(Depth):\n",
        "            #  return [delta[i].size() for i in range(Depth)]\n",
        "            #print('images shape delta, results of Generator:', get_shapes(depth__) )\n",
        "            #print(\"delta last batch\",delta[-1].size())\n",
        "\n",
        "            delta=delta[::-1] # inverse List\n",
        "            x_fake=[torch.tanh(x_real_d[i] + delta[i]) for i in range(len(x_real_d))] # of 1 image 4x3x1024x1024\n",
        "            x_fake=[x_fake[i].detach() for i in range(len(x_real_d))]\n",
        "            out_src, out_cls = self.D(x_fake)\n",
        "            d_loss_fake = torch.mean(out_src)\n",
        "            #print(\"Loss with fake images\",time.time()-t0)\n",
        "            \n",
        "            #print(\"out_src\",out_src.size()) # added to test the dimensions\n",
        "            #print(\"out_cls\",out_cls.size()) # added to test the dimensions\n",
        "            #print(\"d_loss_fake\",d_loss_fake.size()) # added to test the dimensions\n",
        "  \n",
        "\n",
        "          # Compute loss for gradient penalty.\n",
        "            t0=time.time()\n",
        "            alpha = torch.rand(x_real.size(0), 1, 1, 1).to(self.device) # x_real.size(0) is the nb of batch\n",
        "            #x_hat = (alpha * x_real.data + (1 - alpha) * x_fake.data).requires_grad_(True) \n",
        "            x_hat = [(alpha * x_real_d[i].data + (1 - alpha) * x_fake[i].data).requires_grad_(True) for i in range(len(x_real_d))]\n",
        "            out_src, _ = self.D(x_hat)\n",
        "            d_loss_gp = self.gradient_penalty(out_src, x_hat)\n",
        "            #print(\"Loss for gredient penalty\",time.time()-t0)\n",
        "            \n",
        "            #check the gpu before changing\n",
        "            #print(d_loss_gp.get_device())\n",
        "\n",
        "            #print(\"alpha, compute gradient penalty\",alpha) # added to test the dimensions\n",
        "            #print(\"out_src\",out_src.size()) # added to test the dimensions\n",
        "            #print(\"d_loss_gp\",d_loss_gp) # added to test the dimensions\n",
        "    \n",
        "\n",
        "            # Backward and optimize.#\n",
        "            d_loss = d_loss_real + d_loss_fake + self.lambda_cls * d_loss_cls + self.lambda_gp * d_loss_gp \n",
        "\n",
        "            #print(\"d_loss\",d_loss) # added to test the dimensions\n",
        "            \n",
        "            t0=time.time()\n",
        "            self.reset_grad()\n",
        "\n",
        "            if (self.device.type == 'cuda') and (self.ngpu > 1):\n",
        "              d_loss.mean().backward()\n",
        "            else:\n",
        "              d_loss.backward()\n",
        "\n",
        "            self.d_optimizer.step()\n",
        "            #print(\"back propagation and optimization\",time.time()-t0)\n",
        "            # Logging.\n",
        "            loss = {}\n",
        "            loss['D/loss_real'] = d_loss_real.item()\n",
        "            loss['D/loss_fake'] = d_loss_fake.item()\n",
        "            loss['D/loss_cls'] = d_loss_cls.item()\n",
        "            loss['D/loss_gp'] = d_loss_gp.item()\n",
        "            # added\n",
        "            loss['d_loss'] = d_loss.item()\n",
        "            \n",
        "            # =================================================================================== #\n",
        "            #                               3. Train the generator                                #\n",
        "            # =================================================================================== #\n",
        "            \n",
        "            if (i+1) % self.n_critic == 0:\n",
        "\n",
        "                # Original-to-target domain.\n",
        "                t0=time.time()\n",
        "                delta = self.G(x_real, c_trg)\n",
        "                delta=delta[::-1]\n",
        "                x_fake=[torch.tanh(x_real_d[i] + delta[i]) for i in range(len(x_real_d))] # of 1 image 4x3x1024x1024\n",
        "                #x_fake=[x_fake[i].detach() for i in range(len(x_real_d))]\n",
        "                out_src, out_cls = self.D(x_fake)\n",
        "                #x_fake = torch.tanh(x_real_d + delta)\n",
        "                #out_src, out_cls = self.D(x_fake)\n",
        "                g_loss_fake = - torch.mean(out_src)\n",
        "                g_loss_cls = self.classification_loss(out_cls, label_trg, self.dataset)\n",
        "                \n",
        "                #print(\"Original-to-target domain : Generator\",time.time()-t0)\n",
        "\n",
        "                #print(\"delta, generator\",delta.size()) # added to test the dimensions\n",
        "                #print(\"x_fake: tanh(x_real + delta)\",x_fake.size()) # added to test the dimensions\n",
        "                #print(\"out_src\",out_src.size()) # added to test the dimensions\n",
        "                #print(\"out_cls\",out_cls.size()) # added to test the dimensions\n",
        "                #print(\"g_loss_fake\",g_loss_fake) # added to test the dimensions\n",
        "                #print(\"g_loss_cls\",g_loss_cls) # added to test the dimensions\n",
        "\n",
        "\n",
        "\n",
        "                # Original-to-original domain.\n",
        "                t0=time.time()\n",
        "\n",
        "                delta_id = self.G(x_real, c_org)\n",
        "                delta_id=delta_id[::-1]\n",
        "                #x_fake_id = torch.tanh(x_real_d + delta_id)\n",
        "                x_fake_id=[torch.tanh(x_real_d[i] + delta_id[i]) for i in range(len(x_real_d))]\n",
        "                out_src_id, out_cls_id = self.D(x_fake_id)\n",
        "                g_loss_fake_id = - torch.mean(out_src_id)\n",
        "                g_loss_cls_id = self.classification_loss(out_cls_id, label_org, self.dataset)\n",
        "                #g_loss_id = torch.mean(torch.abs(x_real - torch.tanh(delta_id + x_real)))\n",
        "\n",
        "                \n",
        "                def image_padding(ListofTensorImages):\n",
        "                  out=[]\n",
        "                  for img in ListofTensorImages:\n",
        "                    #print(img.size())\n",
        "                    equ= (1024 - img.size(-1))/2\n",
        "                    #print(equ)\n",
        "                    m = nn.ZeroPad2d(int(equ))\n",
        "                    out.append(m(img))\n",
        "                  return out\n",
        "\n",
        "                #g_loss_id = torch.mean(torch.stack(image_padding([torch.abs(x_real_d[i] - x_fake_id[i]) for i in range(len(x_real_d))]))) # 5 scaled images\n",
        "                g_loss_id = torch.mean(torch.abs(x_real_d[0] - x_fake_id[0])) # 1 1024x1024 image\n",
        "                #print(\"Original-to-original domain : Generator\",time.time()-t0)\n",
        "\n",
        "                #print(\"delta_id, generator\",delta_id.size()) # added to test the dimensions\n",
        "                #print(\"x_fake_id: tanh(x_real + delta_id)\",x_fake_id.size()) # added to test the dimensions\n",
        "                #print(\"out_src_id\",out_src_id.size()) # added to test the dimensions\n",
        "                #print(\"out_cls_id\",out_cls_id.size()) # added to test the dimensions\n",
        "                #print(\"g_loss_fake_id\",g_loss_fake_id) # added to test the dimensions\n",
        "                #print(\"g_loss_cls_id\",g_loss_cls_id) # added to test the dimensions\n",
        "                #print(\"g_loss_id\",g_loss_id) # added to test the dimensions\n",
        "\n",
        "\n",
        "                # Target-to-original domain.\n",
        "                t0=time.time()\n",
        "\n",
        "                #print(\"x_fake[-1] size: \",x_fake[0].size()) # added to test the dimensions\n",
        "                delta_reconst = self.G(x_fake[0], c_org)\n",
        "                delta_reconst=delta_reconst[::-1]\n",
        "                #print(\"delta_reconst[-1] size: \",delta_reconst[-1].size()) # added to test the dimensions\n",
        "\n",
        "\n",
        "                x_reconst=[torch.tanh(x_fake[i] + delta_reconst[i]) for i in range(len(x_real_d))]\n",
        "                #x_reconst = torch.tanh(x_fake + delta_reconst)\n",
        "                #g_loss_rec = torch.mean(torch.abs(x_real - x_reconst))\n",
        "                \n",
        "                #g_loss_rec= torch.mean(torch.stack(image_padding([torch.abs(x_real_d[i] - x_reconst[i]) for i in range(len(x_real_d))]))) # 5 scaled images\n",
        "                g_loss_rec= torch.mean(torch.abs(x_real_d[0] - x_reconst[0])) # 1 1024x1024 image\n",
        "\n",
        "\n",
        "                #print(\"delta_reconst, generator\",delta_reconst.size()) # added to test the dimensions\n",
        "                #print(\"x_reconst: tanh(x_real + delta_reconst)\",x_reconst.size()) # added to test the dimensions\n",
        "                #print(\"g_loss_rec\",g_loss_rec) # added to test the dimensions\n",
        "\n",
        "                #print(\"Target-to-original domain : Generator\",time.time()-t0)\n",
        "\n",
        "                # Original-to-original domain.\n",
        "                t0=time.time()\n",
        "\n",
        "                delta_reconst_id = self.G(x_fake_id[0], c_org) \n",
        "                delta_reconst_id = delta_reconst_id[::-1]\n",
        "                x_reconst_id=[torch.tanh(x_fake_id[i] + delta_reconst_id[i]) for i in range(len(x_real_d))]\n",
        "                #x_reconst_id = torch.tanh(x_fake_id + delta_reconst_id)\n",
        "                #g_loss_rec_id = torch.mean(torch.abs(x_real - x_reconst_id))\n",
        "\n",
        "                #g_loss_rec_id= torch.mean(torch.stack(image_padding([torch.abs(x_real_d[i] - x_reconst_id[i]) for i in range(len(x_real_d))])))  # 5 scales images \n",
        "                g_loss_rec_id= torch.mean(torch.abs(x_real_d[0] - x_reconst_id[0]))   # 1 1024x1024 images \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                #print(\"delta_reconst_id, generator\",delta_reconst_id.size()) # added to test the dimensions\n",
        "                #print(\"x_reconst_id: tanh(x_real + delta_reconst_id)\",x_reconst_id.size()) # added to test the dimensions\n",
        "                #print(\"g_loss_rec_id\",g_loss_rec_id) # added to test the dimensions\n",
        "\n",
        "                #print(\"Original-to-original domain : Generator\",time.time()-t0)\n",
        "\n",
        "\n",
        "                # Backward and optimize.\n",
        "                g_loss_same = g_loss_fake_id + self.lambda_rec * g_loss_rec_id + self.lambda_cls * g_loss_cls_id + self.lambda_id * g_loss_id\n",
        "                g_loss = g_loss_fake + self.lambda_rec * g_loss_rec + self.lambda_cls * g_loss_cls + g_loss_same\n",
        "\n",
        "                #print(\"g_loss_same\",g_loss_same.size()) # added to test the dimensions\n",
        "                #print(\"g_loss\",g_loss) # added to test the dimensions\n",
        "\n",
        "                t0=time.time()\n",
        "                self.reset_grad()\n",
        "                #g_loss.backward()\n",
        "\n",
        "                if (self.device.type == 'cuda') and (self.ngpu > 1):\n",
        "                  g_loss.mean().backward()\n",
        "                else:\n",
        "                  g_loss.backward()\n",
        "\n",
        "                self.g_optimizer.step()\n",
        "                #print(\"back Propagation: Generator\",time.time()-t0)\n",
        "\n",
        "                # Logging.\n",
        "                loss['G/loss_fake'] = g_loss_fake.item()\n",
        "                loss['G/loss_rec'] = g_loss_rec.item()\n",
        "                loss['G/loss_cls'] = g_loss_cls.item()\n",
        "                loss['G/loss_fake_id'] = g_loss_fake_id.item()\n",
        "                loss['G/loss_rec_id'] = g_loss_rec_id.item()\n",
        "                loss['G/loss_cls_id'] = g_loss_cls_id.item()\n",
        "                loss['G/loss_id'] = g_loss_id.item()\n",
        "                # added\n",
        "                loss['g_loss'] = g_loss.item()\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                                 4. Miscellaneous                                    #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # Print out training information.\n",
        "            if (i+1) % self.log_step == 0:\n",
        "                et = time.time() - start_time\n",
        "                et = str(datetime.timedelta(seconds=et))[:-7]\n",
        "                log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, i+1, self.num_iters)\n",
        "                for tag, value in loss.items():\n",
        "                    log += \", {}: {:.4f}\".format(tag, value)\n",
        "                print(log,flush=True)\n",
        "\n",
        "                if self.use_tensorboard:\n",
        "                    for tag, value in loss.items():\n",
        "                        self.logger.scalar_summary(tag, value, i+1)\n",
        "\n",
        "            # Translate fixed images for debugging.\n",
        "            if (i+1) % self.sample_step == 0:\n",
        "              from torch.nn.functional import avg_pool2d\n",
        "              #batch_images_fixed=x_fixed  \n",
        "              #depth__=5\n",
        "              #batch_images_fixed = [batch_images_fixed] + [avg_pool2d(batch_images_fixed, int(np.power(2, i)))\n",
        "              #                       for i in range(1, depth__)]\n",
        "              with torch.no_grad():\n",
        "                  x_fake_list = [x_fixed]\n",
        "                  print(\"fake_list[0]\",x_fake_list[0].size()) # 3x3x1024x1024   \n",
        "                  for c_fixed in c_fixed_list:\n",
        "                      delta = self.G(x_fixed, c_fixed)\n",
        "                      delta = delta[::-1]\n",
        "                      gen_img=torch.tanh(delta[0] + x_fixed)\n",
        "                      x_fake_list.append(gen_img) # add to list\n",
        "                  x_concat = torch.cat(x_fake_list, dim=3)\n",
        "                  sample_path = os.path.join(self.sample_dir, '{}-images.jpg'.format(i+1))\n",
        "                  save_image(self.denorm(x_concat.data.cpu()), sample_path, nrow=1, padding=0)\n",
        "                  print('Saved real and fake images into {}...'.format(sample_path))\n",
        "\n",
        "\n",
        "                  # create a grid of samples and save it\n",
        "                  #reses = [str(int(np.power(2, dep+4))) + \"_x_\"\n",
        "                  #+ str(int(np.power(2, dep+4)))\n",
        "                  #for dep in reversed(range(2,depth__ + 2))]\n",
        "\n",
        "                  #gen_img_files = [os.path.join(self.sample_dir, res, \"gen_\" +\n",
        "                  #                              str(i) + \".png\")\n",
        "                  #                  for res in reses]\n",
        "\n",
        "                  # Make sure all the required directories exist\n",
        "                  # otherwise make them\n",
        "                  #os.makedirs(self.sample_dir, exist_ok=True)\n",
        "                  #for gen_img_file in gen_img_files:\n",
        "                  #    os.makedirs(os.path.dirname(gen_img_file), exist_ok=True)\n",
        "\n",
        "                  # fake list should contain the x_fake_list of different versions\n",
        "                  #print(\"x_fixed\",x_fixed[0].size()) \n",
        "                  #print(\"delta\",delta[0].size())      \n",
        "                  #gen_imgs=[torch.tanh(batch_images_fixed[i] + delta[i]) for i in range(len(x_real_d))]\n",
        "                  #self.create_grid(gen_imgs,gen_img_files)\n",
        "\n",
        "            # Save model checkpoints.\n",
        "            if (i+1) % self.model_save_step == 0:\n",
        "                G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(i+1))\n",
        "                D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(i+1))\n",
        "                \n",
        "                if (self.device.type == 'cuda') and (self.ngpu > 1):\n",
        "                  torch.save(self.G.module.state_dict(), G_path) # loading doesn't change\n",
        "                  torch.save(self.D.module.state_dict(), D_path) # loading doesn't change\n",
        "                else:\n",
        "                  torch.save(self.G.state_dict(), G_path)\n",
        "                  torch.save(self.D.state_dict(), D_path)\n",
        "\n",
        "                print('Saved model checkpoints into {}...'.format(self.model_save_dir))\n",
        "\n",
        "            # Decay learning rates.\n",
        "            if (i+1) % self.lr_update_step == 0 and (i+1) > (self.num_iters - self.num_iters_decay):\n",
        "                g_lr = g_lr - (self.g_lr / float(self.num_iters_decay))\n",
        "                d_lr = d_lr - (self.d_lr / float(self.num_iters_decay))\n",
        "                #original\n",
        "                #g_lr -= (self.g_lr / float(self.num_iters_decay))\n",
        "                #d_lr -= (self.d_lr / float(self.num_iters_decay))\n",
        "                self.update_lr(g_lr, d_lr)\n",
        "                print ('Decayed learning rates, g_lr: {}, d_lr: {}.'.format(g_lr, d_lr))\n",
        "\n",
        "\n",
        "    def test(self):\n",
        "        \"\"\"Translate images using Fixed-Point GAN trained on a single dataset.\"\"\"\n",
        "        # Load the trained generator.\n",
        "        self.restore_model(self.test_iters)\n",
        "        \n",
        "        # Set data loader.\n",
        "        if self.dataset in ['CelebA', 'Directory']:\n",
        "            data_loader = self.data_loader\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for i, (x_real, c_org) in enumerate(data_loader):\n",
        "\n",
        "                # Prepare input images and target domain labels.\n",
        "                x_real = x_real.to(self.device)\n",
        "                c_trg_list = self.create_labels(c_org, self.c_dim, self.dataset, self.selected_attrs)\n",
        "\n",
        "                # Translate images.\n",
        "                x_fake_list = [x_real]\n",
        "                for c_trg in c_trg_list:\n",
        "                    x_fake_list.append(torch.tanh(x_real + self.G(x_real, c_trg)))\n",
        "\n",
        "                # Save the translated images.\n",
        "                x_concat = torch.cat(x_fake_list, dim=3)\n",
        "                result_path = os.path.join(self.result_dir, '{}-images.jpg'.format(i+1))\n",
        "                save_image(self.denorm(x_concat.data.cpu()), result_path, nrow=1, padding=0)\n",
        "                print('Saved real and fake images into {}...'.format(result_path))\n",
        "\n",
        "\n",
        "\n",
        "    def test_brats(self):\n",
        "        \"\"\"Translate images using Fixed-Point GAN trained on a single dataset.\"\"\"\n",
        "        # Load the trained generator.\n",
        "        self.restore_model(self.test_iters)\n",
        "        \n",
        "        # Set data loader.\n",
        "        if self.dataset in ['BRATS']:\n",
        "            data_loader = self.data_loader\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for i, (x_real, c_org) in enumerate(data_loader):\n",
        "                x_real = x_real.to(self.device)\n",
        "\n",
        "                c_trg = c_org.clone()\n",
        "                c_trg[:, 0] = 0 # always to healthy              \n",
        "                c_trg_list = [c_trg.to(self.device)]\n",
        "\n",
        "                # Translate images.\n",
        "                x_fake_list = [x_real]\n",
        "                for c_trg in c_trg_list:\n",
        "                    delta = self.G(x_real, c_trg)\n",
        "                    delta_org = torch.abs(torch.tanh(delta + x_real) - x_real) - 1.0\n",
        "                    delta_gray = np.mean(delta_org.data.cpu().numpy(), axis=1)\n",
        "                    delta_gray_norm = []\n",
        "\n",
        "                    loc = []\n",
        "                    cls_mul = []\n",
        "\n",
        "                    for indx in range(delta_gray.shape[0]):\n",
        "                        temp = delta_gray[indx, :, :] + 1.0  \n",
        "                        tempimg_th = np.percentile(temp, 99)\n",
        "                        tempimg = np.float32(temp >= tempimg_th)\n",
        "                        temploc = np.reshape(tempimg, (self.image_size*self.image_size, 1))\n",
        "\n",
        "                        kmeans = KMeans(n_clusters=2, random_state=0).fit(temploc)\n",
        "                        labels = kmeans.predict(temploc)\n",
        "\n",
        "                        recreated_loc = self.recreate_image(kmeans.cluster_centers_, labels, self.image_size, self.image_size)\n",
        "                        recreated_loc = ((recreated_loc - np.min(recreated_loc)) / (np.max(recreated_loc) - np.min(recreated_loc)))\n",
        "\n",
        "                        loc.append(recreated_loc)\n",
        "                        delta_gray_norm.append( tempimg )\n",
        "\n",
        "\n",
        "                    loc = np.array(loc, dtype=np.float32)[:, :, :, 0]\n",
        "                    delta_gray_norm = np.array(delta_gray_norm)\n",
        "\n",
        "                    loc = (loc * 2.0) - 1.0\n",
        "                    delta_gray_norm = (delta_gray_norm * 2.0) - 1.0\n",
        "\n",
        "                    x_fake_list.append( torch.from_numpy(np.repeat(delta_gray[:, np.newaxis, :, :], 3, axis=1)).to(self.device) ) # difference map\n",
        "                    x_fake_list.append( torch.from_numpy(np.repeat(delta_gray_norm[:, np.newaxis, :, :], 3, axis=1)).to(self.device) ) # localization thershold\n",
        "                    x_fake_list.append( torch.from_numpy(np.repeat(loc[:, np.newaxis, :, :], 3, axis=1)).to(self.device) ) # localization kmeans\n",
        "                    x_fake_list.append( torch.tanh(delta + x_real) ) # generated image\n",
        "\n",
        "                # Save the translated images.\n",
        "                x_concat = torch.cat(x_fake_list, dim=3)\n",
        "                result_path = os.path.join(self.result_dir, '{}-images.jpg'.format(i+1))\n",
        "                save_image(self.denorm(x_concat.data.cpu()), result_path, nrow=1, padding=0)\n",
        "                print('Saved real and fake images into {}...'.format(result_path))\n",
        "\n",
        "\n",
        "\n",
        "    def cleanup():\n",
        "      dist.destroy_process_group()   \n",
        "\n",
        "# Solver               \n",
        "\n",
        "from torch.utils import data\n",
        "from torchvision import transforms as T\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "class CelebA(data.Dataset):\n",
        "    \"\"\"Dataset class for the CelebA dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, image_dir, attr_path, selected_attrs, transform, mode):\n",
        "        \"\"\"Initialize and preprocess the CelebA dataset.\"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.attr_path = attr_path\n",
        "        self.selected_attrs = selected_attrs\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        self.train_dataset = []\n",
        "        self.test_dataset = []\n",
        "        self.attr2idx = {}\n",
        "        self.idx2attr = {}\n",
        "        self.preprocess()\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.num_images = len(self.train_dataset)\n",
        "        else:\n",
        "            self.num_images = len(self.test_dataset)\n",
        "\n",
        "    def preprocess(self):\n",
        "        \"\"\"Preprocess the CelebA attribute file.\"\"\"\n",
        "        lines = [line.rstrip() for line in open(self.attr_path, 'r')]\n",
        "        all_attr_names = lines[1].split()\n",
        "        for i, attr_name in enumerate(all_attr_names):\n",
        "            self.attr2idx[attr_name] = i\n",
        "            self.idx2attr[i] = attr_name\n",
        "\n",
        "        lines = lines[2:]\n",
        "        random.seed(1234)\n",
        "        random.shuffle(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            split = line.split()\n",
        "            filename = split[0]\n",
        "            values = split[1:]\n",
        "\n",
        "            label = []\n",
        "            for attr_name in self.selected_attrs:\n",
        "                idx = self.attr2idx[attr_name]\n",
        "                label.append(values[idx] == '1')\n",
        "\n",
        "            if (i+1) < 2000:\n",
        "                self.test_dataset.append([filename, label])\n",
        "            else:\n",
        "                self.train_dataset.append([filename, label])\n",
        "\n",
        "        print('Finished preprocessing the CelebA dataset...')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Return one image and its corresponding attribute label.\"\"\"\n",
        "        dataset = self.train_dataset if self.mode == 'train' else self.test_dataset\n",
        "        filename, label = dataset[index]\n",
        "\n",
        "        # when gcp is used \n",
        "        os.environ['GCLOUD_PROJECT'] = config.GCP_project_id\n",
        "        Bucket_name=config.GCP_Bucket_name\n",
        "        # Use Key authentication\n",
        "        storage_client = storage.Client()\n",
        "        # create bucket\n",
        "        bucket = storage_client.get_bucket(Bucket_name)\n",
        "\n",
        "        # 'data/celeba/images/'\n",
        "        # data_hd/CelebA/\n",
        "        blob = bucket.blob('data_hd/CelebA/'+filename)\n",
        "        filename__=blob.download_as_string()\n",
        "        image=Image.open(BytesIO(filename__))\n",
        "\n",
        "        # when gcp is not used\n",
        "        #image = Image.open(os.path.join(self.image_dir, filename))\n",
        "        return self.transform(image), torch.FloatTensor(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of images.\"\"\"\n",
        "        return self.num_images\n",
        "\n",
        "\n",
        "class BRATS_SYN(data.Dataset):\n",
        "    \"\"\"Dataset class for the BRATS dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, image_dir, transform, mode):\n",
        "        \"\"\"Initialize and Load the BRATS dataset.\"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        self.train_dataset = []\n",
        "        self.test_dataset = []\n",
        "        self.load_data()\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.num_images = len(self.train_dataset)\n",
        "        else:\n",
        "            self.num_images = len(self.test_dataset)\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load BRATS dataset\"\"\"\n",
        "        \n",
        "        # Load test dataset\n",
        "        test_neg = glob(os.path.join(self.image_dir, 'test', 'negative', '*jpg'))\n",
        "        test_pos = glob(os.path.join(self.image_dir, 'test', 'positive', '*jpg'))\n",
        "\n",
        "        for filename in test_neg:\n",
        "            self.test_dataset.append([filename, [0]])\n",
        "\n",
        "        for filename in test_pos:\n",
        "            self.test_dataset.append([filename, [1]])\n",
        "\n",
        "\n",
        "        # Load train dataset\n",
        "        train_neg = glob(os.path.join(self.image_dir, 'train', 'negative', '*jpg'))\n",
        "        train_pos = glob(os.path.join(self.image_dir, 'train', 'positive', '*jpg'))\n",
        "\n",
        "        for filename in train_neg:\n",
        "            self.train_dataset.append([filename, [0]])\n",
        "\n",
        "        for filename in train_pos:\n",
        "            self.train_dataset.append([filename, [1]])\n",
        "\n",
        "        print('Finished loading the BRATS dataset...')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Return one image and its corresponding attribute label.\"\"\"\n",
        "        dataset = self.train_dataset if self.mode == 'train' else self.test_dataset\n",
        "        filename, label = dataset[index]\n",
        "        image = Image.open(filename)\n",
        "        return self.transform(image), torch.FloatTensor(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of images.\"\"\"\n",
        "        return self.num_images\n",
        "\n",
        "\n",
        "def get_loader(image_dir, attr_path, selected_attrs, crop_size=178, image_size=128, \n",
        "               batch_size=16, dataset='CelebA', mode='train', num_workers=1):\n",
        "    \"\"\"Build and return a data loader.\"\"\"\n",
        "    transform = []\n",
        "    if mode == 'train':\n",
        "        transform.append(T.RandomHorizontalFlip())\n",
        "    transform.append(T.CenterCrop(crop_size))\n",
        "    transform.append(T.Resize(image_size))\n",
        "    transform.append(T.ToTensor())\n",
        "    transform.append(T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))\n",
        "    transform = T.Compose(transform)\n",
        "\n",
        "    if dataset == 'CelebA':\n",
        "        dataset =  CelebA(image_dir, attr_path, selected_attrs, transform, mode)\n",
        "    elif dataset == 'BRATS':\n",
        "        dataset = BRATS_SYN(image_dir, transform, mode)\n",
        "    elif dataset == 'Directory':\n",
        "        dataset = ImageFolder(image_dir, transform)\n",
        "\n",
        "    data_loader = data.DataLoader(dataset=dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=(mode=='train'),\n",
        "                                  num_workers=num_workers)\n",
        "    return data_loader\n",
        "\n",
        "                                \n",
        "                \n",
        "# Main\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "#from solver import Solver\n",
        "#from data_loader import get_loader\n",
        "from torch.backends import cudnn\n",
        "from google.cloud import storage\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from io import BytesIO\n",
        "#from PIL import Image\n",
        "import zipfile\n",
        "from glob import glob\n",
        "from tensorboard import version\n",
        "\n",
        "# detect directory\n",
        "# current directory: path at terminal when executing this file\n",
        "cwd = os.getcwd()\n",
        "print(\"Curret Directory: \",cwd)\n",
        "\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in ('true')\n",
        "\n",
        "def main(config):\n",
        "    # For fast training.\n",
        "    cudnn.benchmark = True #True\n",
        "    cudnn.enabled = True # added\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "\n",
        "    # Create directories if not exist.\n",
        "    if not os.path.exists(config.log_dir):\n",
        "        os.makedirs(config.log_dir)\n",
        "    print(os.path.exists(config.log_dir))\n",
        "    if not os.path.exists(config.model_save_dir):\n",
        "        os.makedirs(config.model_save_dir)\n",
        "    print(os.path.exists(config.model_save_dir))\n",
        "    if not os.path.exists(config.sample_dir):\n",
        "        os.makedirs(config.sample_dir)\n",
        "    print(os.path.exists(config.sample_dir))\n",
        "    if not os.path.exists(config.result_dir):\n",
        "        os.makedirs(config.result_dir)\n",
        "    print(os.path.exists(config.result_dir))\n",
        "\n",
        "    #os.environ['GCLOUD_PROJECT'] = config.GCP_project_id\n",
        "    Bucket_name=config.GCP_Bucket_name\n",
        "\n",
        "    # Use Key authentication\n",
        "    #storage_client = storage.Client()\n",
        "\n",
        "    # create bucket\n",
        "    #bucket = storage_client.get_bucket(Bucket_name)\n",
        "\n",
        "    # read dataset.zip from bucket and copy it to directory\n",
        "    #t0=time.time()\n",
        "    #for blob in bucket.list_blobs(prefix= config.dataset_in_cloud):\n",
        "    #  print(blob.name)\n",
        "    #  tempfile= cwd + \"/dataset__gcp.zip\"\n",
        "    #  blob.download_to_filename(tempfile)\n",
        "    #t_end_zip=time.time()-t0\n",
        "    #print('dataset gcp moved to local')\n",
        "\n",
        "    #time.sleep(1)\n",
        "\n",
        "    # read from directort zip file and extract\n",
        "    #t0=time.time()\n",
        "    #with zipfile.ZipFile(\"dataset__gcp.zip\",\"r\") as zip_ref:\n",
        "    #    zip_ref.extractall(config.dataset_folder_name)\n",
        "    #t_end_unzip=time.time()-t0\n",
        "    #print('Extraction completed')\n",
        "\n",
        "    #time.sleep(1)\n",
        "\n",
        "\n",
        "        \n",
        "    # Data loader.\n",
        "    data_loader = None\n",
        "\n",
        "    # 'data/celeba/images'\n",
        "    # config.image_dir\n",
        "\n",
        "    if config.dataset in ['CelebA']:#CelebA\n",
        "        data_loader = get_loader(config.image_dir, config.attr_path, config.selected_attrs,\n",
        "                                   config.crop_size, config.image_size, config.batch_size,\n",
        "                                   'CelebA', config.mode, config.num_workers)\n",
        "\n",
        "\n",
        "    elif config.dataset in ['BRATS']:\n",
        "        data_loader = get_loader('{}/brats/syn'.format(config.dataset_folder_name), None, None,\n",
        "                                   config.crop_size, config.image_size, config.batch_size,\n",
        "                                   'BRATS', config.mode, config.num_workers)\n",
        "\n",
        "\n",
        "    elif config.dataset in ['Directory']:\n",
        "        data_loader = get_loader('config.image_dir', None, None,\n",
        "                                 config.crop_size, config.image_size, config.batch_size,\n",
        "                                 'Directory', config.mode, config.num_workers)\n",
        "\n",
        "        \n",
        "    # Solver for training and testing Fixed-Point GAN.\n",
        "    solver = Solver(data_loader, config)\n",
        "\n",
        "    if config.mode == 'train':\n",
        "        if config.dataset in ['CelebA', 'BRATS', 'Directory']:  #CelebA\n",
        "            solver.train()\n",
        "    elif config.mode == 'test':\n",
        "        if config.dataset in ['CelebA', 'Directory']:\n",
        "            solver.test()\n",
        "    elif config.mode == 'test_brats':\n",
        "        if config.dataset in ['BRATS']:\n",
        "            solver.test_brats()\n",
        "\n",
        "    def Copy_local_dir_to_GCP_bucket(directory,folder_name_to_be_copied,gcp_output_folder):\n",
        "        \n",
        "        # to be later on fixed to be controlled from the main\n",
        "        os.environ['GCLOUD_PROJECT'] = config.GCP_project_id\n",
        "        Bucket_name=config.GCP_Bucket_name\n",
        "        # Use Key authentication\n",
        "        storage_client = storage.Client()\n",
        "        # create bucket\n",
        "        bucket = storage_client.get_bucket(Bucket_name)\n",
        "\n",
        "        # Get the list of all files in directory tree at given path\n",
        "        path = directory + \"/\" + folder_name_to_be_copied + \"/\"\n",
        "        #we shall store all the file names in this list\n",
        "        filelist = []\n",
        "\n",
        "        for root, dirs, files in os.walk(path):\n",
        "          for file in files:\n",
        "            #append the file name to the list\n",
        "            filelist.append(os.path.join(root,file))\n",
        "\n",
        "        # write files and subdirectories to gcp\n",
        "        for name in filelist:\n",
        "            # print(name)\n",
        "            blob=bucket.blob(name.replace(directory+\"/\" , gcp_output_folder+\"/\"))\n",
        "            blob.upload_from_filename(name)\n",
        "        return print('Output were transfered to gcp bucket')\n",
        "\n",
        "\n",
        "    if config.dataset == 'BRATS':\n",
        "      t0=time.time()\n",
        "      Copy_local_dir_to_GCP_bucket(cwd,'brats_syn_256_lambda0.1', bucket,\"Output\")\n",
        "      t_end_copy_folder_to_gcp=time.time()-t0\n",
        "      print('time to end copying folders to gcp bucket',t_end_copy_folder_to_gcp)\n",
        "    elif config.dataset == 'CelebA': # CelebA\n",
        "      t0=time.time()\n",
        "      Copy_local_dir_to_GCP_bucket(cwd,'celeba_1',\"Output\")\n",
        "      t_end_copy_folder_to_gcp=time.time()-t0\n",
        "      print('time to end copying folders to gcp bucket',t_end_copy_folder_to_gcp)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Model configuration.\n",
        "    parser.add_argument('--c_dim', type=int, default=5, help='dimension of domain labels (1st dataset)')\n",
        "    parser.add_argument('--c2_dim', type=int, default=8, help='dimension of domain labels (2nd dataset)')\n",
        "    parser.add_argument('--crop_size', type=int, default=1024, help='crop size for the images') # was 178\n",
        "    parser.add_argument('--image_size', type=int, default=128, help='image resolution')\n",
        "    parser.add_argument('--g_conv_dim', type=int, default=16, help='number of conv filters in the first layer of G') # was 64\n",
        "    parser.add_argument('--d_conv_dim', type=int, default=16, help='number of conv filters in the first layer of D') # was 64\n",
        "    parser.add_argument('--g_repeat_num', type=int, default=8, help='number of residual blocks in G') # ws 6\n",
        "    parser.add_argument('--d_repeat_num', type=int, default=8, help='number of strided conv layers in D') # was 6\n",
        "    parser.add_argument('--lambda_cls', type=float, default=1, help='weight for domain classification loss')\n",
        "    parser.add_argument('--lambda_rec', type=float, default=10, help='weight for reconstruction loss')\n",
        "    parser.add_argument('--lambda_gp', type=float, default=10, help='weight for gradient penalty')\n",
        "    parser.add_argument('--lambda_id', type=float, default=10, help='weight for identity loss')\n",
        "    parser.add_argument('--depth', type=int, default=5, help='Number of output images/layers')\n",
        "\n",
        "    # Training configuration.\n",
        "    parser.add_argument('--dataset', type=str, default='CelebA', choices=['CelebA', 'BRATS', 'Directory'])#CelebA\n",
        "    parser.add_argument('--batch_size', type=int, default=16, help='mini-batch size')\n",
        "    parser.add_argument('--num_iters', type=int, default=200000, help='number of total iterations for training D')\n",
        "    parser.add_argument('--num_iters_decay', type=int, default=100000, help='number of iterations for decaying lr')\n",
        "    parser.add_argument('--g_lr', type=float, default=0.0001, help='learning rate for G')\n",
        "    parser.add_argument('--d_lr', type=float, default=0.0001, help='learning rate for D')\n",
        "    parser.add_argument('--n_critic', type=int, default=5, help='number of D updates per each G update')\n",
        "    parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for Adam optimizer')\n",
        "    parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for Adam optimizer')\n",
        "    parser.add_argument('--resume_iters', type=int, default=None, help='resume training from this step')\n",
        "    parser.add_argument('--selected_attrs', '--list', nargs='+', help='selected attributes for the CelebA dataset',\n",
        "                        default=['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young'])\n",
        "\n",
        "    # Test configuration.\n",
        "    parser.add_argument('--test_iters', type=int, default=200000, help='test model from this step')\n",
        "\n",
        "    # Miscellaneous.\n",
        "    parser.add_argument('--num_workers', type=int, default=1)\n",
        "    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test', 'test_brats'])\n",
        "    parser.add_argument('--use_tensorboard', type=str2bool, default=True)\n",
        "    parser.add_argument('--job-dir', help=\" will be passed through from the AI-Platform to your program when you define it.\")\n",
        "    #parser.add_argument('--ngpu', type=int, default=0)\n",
        "    parser.add_argument('--GCP_project_id', type=str, default='fpcityuni-2020')\n",
        "    parser.add_argument('--GCP_Bucket_name', type=str, default=\"fpcityuni-2020-storage\")\n",
        "    \n",
        "    # Directories.\n",
        "    parser.add_argument('--image_dir', type=str, default='data/celeba/images')\n",
        "    \n",
        "    parser.add_argument('--dataset_in_cloud', type=str, default='dataset.zip')\n",
        "    parser.add_argument('--dataset_folder_name', type=str, default='data')\n",
        "    parser.add_argument('--attr_path', type=str, default='data/celeba/list_attr_celeba.txt')\n",
        "    parser.add_argument('--log_dir', type=str, default='celeba/logs')\n",
        "    parser.add_argument('--model_save_dir', type=str, default='celeba/models')\n",
        "    parser.add_argument('--sample_dir', type=str, default='celeba/samples')\n",
        "    parser.add_argument('--result_dir', type=str, default='celeba/results')\n",
        "\n",
        "    # Step size.\n",
        "    parser.add_argument('--log_step', type=int, default=10)\n",
        "    parser.add_argument('--sample_step', type=int, default=1000)\n",
        "    parser.add_argument('--model_save_step', type=int, default=10000)\n",
        "    parser.add_argument('--lr_update_step', type=int, default=1000)\n",
        "\n",
        "    config = parser.parse_args()\n",
        "    print(config, flush='True')\n",
        "    print('__Tensorflow VERSION',tf.__version__)\n",
        "    print('__Tensorboard VERSION:', version.VERSION)\n",
        "    print('__pyTorch VERSION:', torch.__version__)\n",
        "    #import torchvision\n",
        "    #print('__Torchvision VERSION',torchvision.__version__)\n",
        "    print('__CUDA VERSION',torch.version.cuda )\n",
        "    from subprocess import call\n",
        "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
        "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
        "    print('__Devices')\n",
        "    if torch.cuda.device_count() >0:\n",
        "      print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
        "      print('Current device name',torch.cuda.get_device_name(0))\n",
        "    else:\n",
        "      print('No Cuda Device')\n",
        "    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    main(config)\n",
        "    print(torch.cuda.memory_allocated())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting GAN_tf_nGPU_for_GCP_2_test_losses_Method1_LastResolutionLoss.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxIbDnwYOl0P",
        "colab_type": "text"
      },
      "source": [
        "# Method 1 Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHsOIpMiHyp_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67b0c108-09dd-4c6b-eec3-21a3150d72ae"
      },
      "source": [
        "! python3 -u GAN_tf_nGPU_for_GCP_2_test_losses_Method1_LastResolutionLoss.py --mode train --dataset 'CelebA' --image_size 1024 --crop_size 1024 --c_dim 5 --g_conv_dim 16 --d_conv_dim 16 --depth 5 \\\n",
        "                 --GCP_project_id fpcityuni-2020 \\\n",
        "                 --GCP_Bucket_name fpcityuni-2020-storage \\\n",
        "                 --image_dir data/test_max_size_of_readable_images \\\n",
        "                 --attr_path list_attr_celeba_hd.txt \\\n",
        "                 --sample_dir celeba_1/samples \\\n",
        "                 --log_dir celeba_1/logs \\\n",
        "                 --model_save_dir celeba_1/models \\\n",
        "                 --result_dir celeba_1/results \\\n",
        "                 --selected_attrs Black_Hair Blond_Hair Brown_Hair Male Young --lambda_id 10 \\\n",
        "                 --batch_size 3 --num_workers 3 --num_iters 50000 --log_step 50 --sample_step 5000 #--resume_iters 10000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-15 21:15:11.883756: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Curret Directory:  /content/drive/My Drive/FinalProject-GCP\n",
            "Namespace(GCP_Bucket_name='fpcityuni-2020-storage', GCP_project_id='fpcityuni-2020', attr_path='list_attr_celeba_hd.txt', batch_size=3, beta1=0.5, beta2=0.999, c2_dim=8, c_dim=5, crop_size=1024, d_conv_dim=16, d_lr=0.0001, d_repeat_num=8, dataset='CelebA', dataset_folder_name='data', dataset_in_cloud='dataset.zip', depth=5, g_conv_dim=16, g_lr=0.0001, g_repeat_num=8, image_dir='data/test_max_size_of_readable_images', image_size=1024, job_dir=None, lambda_cls=1, lambda_gp=10, lambda_id=10.0, lambda_rec=10, log_dir='celeba_1/logs', log_step=50, lr_update_step=1000, mode='train', model_save_dir='celeba_1/models', model_save_step=10000, n_critic=5, num_iters=50000, num_iters_decay=100000, num_workers=3, result_dir='celeba_1/results', resume_iters=None, sample_dir='celeba_1/samples', sample_step=5000, selected_attrs=['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young'], test_iters=200000, use_tensorboard=True)\n",
            "__Tensorflow VERSION 2.3.0\n",
            "__Tensorboard VERSION: 2.3.0\n",
            "__pyTorch VERSION: 1.6.0+cu101\n",
            "__CUDA VERSION 10.1\n",
            "__CUDNN VERSION: 7603\n",
            "__Number CUDA Devices: 1\n",
            "__Devices\n",
            "Active CUDA Device: GPU 0\n",
            "Current device name Tesla P100-PCIE-16GB\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "Finished preprocessing the CelebA dataset...\n",
            "Number of Cuda devices:  1\n",
            "kernel size 4\n",
            "Generator(\n",
            "  (main_DownSampling_bottleneck): Sequential(\n",
            "    (0): Conv2d(8, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
            "    (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): ReLU(inplace=True)\n",
            "    (15): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (16): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): ResidualBlock(\n",
            "      (main): Sequential(\n",
            "        (0): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (19): ResidualBlock(\n",
            "      (main): Sequential(\n",
            "        (0): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (20): ResidualBlock(\n",
            "      (main): Sequential(\n",
            "        (0): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (21): ResidualBlock(\n",
            "      (main): Sequential(\n",
            "        (0): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (22): ResidualBlock(\n",
            "      (main): Sequential(\n",
            "        (0): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (23): ResidualBlock(\n",
            "      (main): Sequential(\n",
            "        (0): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (24): ResidualBlock(\n",
            "      (main): Sequential(\n",
            "        (0): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (25): ResidualBlock(\n",
            "      (main): Sequential(\n",
            "        (0): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (UpsamplingLayers): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (rgb_converters): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(16, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Generator\n",
            "The number of parameters: 9882143\n",
            "Discriminator(\n",
            "  (rgb_to_features): ModuleList(\n",
            "    (0): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (2): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (3): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (4): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (Dis_blocks): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(35, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (3): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): Conv2d(67, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (3): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): Conv2d(131, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "      (2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (3): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): Conv2d(259, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "      (2): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (3): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "  )\n",
            "  (conv1): Conv2d(2048, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(2048, 5, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (3): LeakyReLU(negative_slope=0.01)\n",
            "    (4): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (5): LeakyReLU(negative_slope=0.01)\n",
            "  )\n",
            ")\n",
            "Discriminator\n",
            "The number of parameters: 45717538\n",
            "2020-08-15 21:15:19.005671: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-15 21:15:19.005985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:15:19.006877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-08-15 21:15:19.006917: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-15 21:15:19.008787: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-15 21:15:19.010652: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-15 21:15:19.011084: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-15 21:15:19.012968: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-15 21:15:19.013765: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-15 21:15:19.017234: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-15 21:15:19.017410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:15:19.018366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:15:19.019231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-15 21:15:19.024932: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-15 21:15:19.025672: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xa8708a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-15 21:15:19.025704: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-15 21:15:19.027180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:15:19.028535: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xa8708bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-15 21:15:19.028573: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-08-15 21:15:19.028865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:15:19.029704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-08-15 21:15:19.029741: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-15 21:15:19.029778: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-15 21:15:19.029811: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-15 21:15:19.029843: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-15 21:15:19.029876: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-15 21:15:19.029904: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-15 21:15:19.029931: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-15 21:15:19.030035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:15:19.031003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:15:19.031842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-15 21:15:19.031905: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-15 21:15:19.031958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-15 21:15:19.031976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-15 21:15:19.031985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-15 21:15:19.032175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:15:19.033120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:15:19.033931: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-15 21:15:19.033985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14265 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "Start training...\n",
            "Elapsed [0:01:26], Iteration [50/50000], D/loss_real: -689.7470, D/loss_fake: 613.7729, D/loss_cls: 2.9979, D/loss_gp: 1.5727, d_loss: -57.2488, G/loss_fake: -632.9399, G/loss_rec: 0.2115, G/loss_cls: 3.0270, G/loss_fake_id: -632.9220, G/loss_rec_id: 0.2111, G/loss_cls_id: 2.9782, G/loss_id: 0.1268, g_loss: -1254.3634\n",
            "Elapsed [0:02:44], Iteration [100/50000], D/loss_real: 46.4498, D/loss_fake: -262.0501, D/loss_cls: 2.9839, D/loss_gp: 0.5473, d_loss: -207.1432, G/loss_fake: -182.8656, G/loss_rec: 0.2239, G/loss_cls: 3.0426, G/loss_fake_id: -183.4708, G/loss_rec_id: 0.2239, G/loss_cls_id: 3.0816, G/loss_id: 0.1447, g_loss: -354.2871\n",
            "Elapsed [0:04:03], Iteration [150/50000], D/loss_real: 68.2626, D/loss_fake: -761.7809, D/loss_cls: 2.9400, D/loss_gp: 9.5185, d_loss: -595.3930, G/loss_fake: 2547.4541, G/loss_rec: 0.1650, G/loss_cls: 3.3228, G/loss_fake_id: 2546.8291, G/loss_rec_id: 0.1650, G/loss_cls_id: 3.1162, G/loss_id: 0.1007, g_loss: 5105.0293\n",
            "Traceback (most recent call last):\n",
            "  File \"GAN_tf_nGPU_for_GCP_2_test_losses_Method1_LastResolutionLoss.py\", line 1599, in <module>\n",
            "    main(config)\n",
            "  File \"GAN_tf_nGPU_for_GCP_2_test_losses_Method1_LastResolutionLoss.py\", line 1471, in main\n",
            "    solver.train()\n",
            "  File \"GAN_tf_nGPU_for_GCP_2_test_losses_Method1_LastResolutionLoss.py\", line 999, in train\n",
            "    g_loss.backward()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 185, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 127, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd5072OpIJA2",
        "colab_type": "text"
      },
      "source": [
        "#Method 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKXM0uRuE_e7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70feb2ee-1e6f-4f40-80d7-f86aa9a6d19e"
      },
      "source": [
        "%%writefile GAN_tf_nGPU_for_GCP_2_test_losses_Method2_scaledimagesaveraging.py\n",
        "\n",
        "# inserting a file with huge number of images doesn't work with google drive, gcp is needed to read the data from there\n",
        "# Model\n",
        "# works with 2 gpu not 3 and with 20 batch size 10.30 mins\n",
        "\n",
        "# Change the code to blocks\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.nn import ModuleList, Conv2d\n",
        "\n",
        "\n",
        "\n",
        "class depthwise_separable_conv(nn.Module):\n",
        "    def __init__(self, nin, nout, kernels_per_layer=1, K_depth=3, S_depth=1, P_depth=1, D_depth=1, K_point=1, S_point=1, P_point=0, D_point=1, bias=False):\n",
        "        super(depthwise_separable_conv, self).__init__()\n",
        "        self.depthwise = nn.Conv2d(nin, nin * kernels_per_layer, kernel_size=K_depth,stride=S_depth, padding=P_depth,dilation=D_depth, groups=nin,bias=bias)\n",
        "        self.pointwise = nn.Conv2d(nin * kernels_per_layer, nout, kernel_size=K_point,stride=S_point,padding=P_point,dilation=D_point, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depthwise(x)\n",
        "        out = self.pointwise(out)\n",
        "        return out\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual Block with instance normalization.\"\"\"\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            depthwise_separable_conv(nin=dim_in,nout=dim_out, kernels_per_layer=1,K_depth=3,S_depth=1,P_depth=1,D_depth=1,K_point=1,S_point=1,P_point=0,D_point=1,bias=False),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            depthwise_separable_conv(nin=dim_in,nout=dim_out, kernels_per_layer=1,K_depth=3,S_depth=1,P_depth=1,D_depth=1,K_point=1,S_point=1,P_point=0,D_point=1,bias=False),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.main(x)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"Generator network.\"\"\"\n",
        "    def __init__(self, ngpu=0, conv_dim=16, c_dim=1, repeat_num=8,depth=5):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = torch.cuda.device_count()\n",
        "        \n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(3+c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False))\n",
        "        layers.append(nn.InstanceNorm2d(conv_dim, affine=True, track_running_stats=True))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        # Down-sampling layers.\n",
        "        curr_dim = conv_dim\n",
        "        for i in range(depth):\n",
        "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False))\n",
        "            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True, track_running_stats=True))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        # Bottleneck layers.\n",
        "        for i in range(repeat_num):\n",
        "            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))\n",
        "        \n",
        "        self.main_DownSampling_bottleneck = nn.Sequential(*layers)\n",
        "        \n",
        "        # create a module list of the other required general convolution blocks\n",
        "        self.UpsamplingLayers = ModuleList()\n",
        "        self.rgb_converters = ModuleList()\n",
        "\n",
        "        # Up-sampling layers.\n",
        "        def conv_block(curr_dim,K=4,S=2,P=1,bias_=False):\n",
        "          return nn.Sequential( nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=K, stride=S, padding=P, bias=bias_),\n",
        "            nn.InstanceNorm2d(curr_dim//2, affine=True, track_running_stats=True),\n",
        "            nn.ReLU(inplace=True))\n",
        "        \n",
        "        # to rgb conversion\n",
        "        def to_rgb(in_channels):\n",
        "          layers_rgb = []\n",
        "          layers_rgb.append(nn.Sequential(Conv2d(in_channels, 3, (1, 1),(1,1),(0,0), bias=True)))\n",
        "          #layers_rgb.append(nn.Tanh())\n",
        "          out = nn.Sequential(*layers_rgb)\n",
        "          return out\n",
        "\n",
        "        for i in range(depth):\n",
        "          if i != (depth-1):\n",
        "            self.UpsamplingLayers.append(conv_block(curr_dim,4,2,1,False))\n",
        "            rgb = to_rgb(int(curr_dim//2))\n",
        "            self.rgb_converters.append(rgb)\n",
        "            curr_dim = curr_dim // 2\n",
        "          else :\n",
        "            self.UpsamplingLayers.append(conv_block(curr_dim,4,2,1,False)) # curr_dim,7,1,3,False)\n",
        "            rgb = to_rgb(int(curr_dim//2))\n",
        "            self.rgb_converters.append(rgb)\n",
        "            curr_dim = curr_dim // 2\n",
        "          \n",
        "\n",
        "    def forward(self, x, c):\n",
        "        # Replicate spatially and concatenate domain information.\n",
        "        c = c.view(c.size(0), c.size(1), 1, 1)\n",
        "        c = c.repeat(1, 1, x.size(2), x.size(3))\n",
        "        x = torch.cat([x, c], dim=1)\n",
        "\n",
        "        #print(self.UpsamplingLayers)\n",
        "        #print(self.rgb_converters)\n",
        "\n",
        "        outputs=[]\n",
        "        Y=self.main_DownSampling_bottleneck(x) # start the computational pipeline\n",
        "        #counter=0\n",
        "        #print(\"Y after downsmapling and bottleneck\",Y.size())\n",
        "        for block, converter in zip(self.UpsamplingLayers, self.rgb_converters):\n",
        "            #print(\"Y again\",Y.size())\n",
        "            Y = block(Y)\n",
        "            outputs.append(converter(Y))\n",
        "            #counter=counter+1\n",
        "            #print(counter)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "      \n",
        "    @staticmethod\n",
        "    def adjust_dynamic_range(data, drange_in=(-1, 1), drange_out=(0, 1)):\n",
        "      \"\"\"\n",
        "      adjust the dynamic colour range of the given input data\n",
        "      :param data: input image data\n",
        "      :param drange_in: original range of input\n",
        "      :param drange_out: required range of output\n",
        "      :return: img => colour range adjusted images\n",
        "      \"\"\"\n",
        "      if drange_in != drange_out:\n",
        "        scale = (np.float32(drange_out[1]) - np.float32(drange_out[0])) / (np.float32(drange_in[1]) - np.float32(drange_in[0]))\n",
        "        bias = (np.float32(drange_out[0]) - np.float32(drange_in[0]) * scale)\n",
        "        data = data * scale + bias\n",
        "      return torch.clamp(data, min=0, max=1)\n",
        "\n",
        "\n",
        "\n",
        "# Discriminator code using blocks\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.nn import ModuleList, Conv2d\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"Discriminator network with PatchGAN.\"\"\"\n",
        "    def __init__(self, ngpu=0, image_size=1024, conv_dim=16, c_dim=5, repeat_num=8,depth=5):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu=torch.cuda.device_count()\n",
        "\n",
        "        def from_rgb(out_channels):\n",
        "          return Conv2d(3, out_channels, (1, 1), bias=True)\n",
        "\n",
        "        curr_dim = conv_dim\n",
        "        self.rgb_to_features = ModuleList()\n",
        "\n",
        "  \n",
        "        # Discrimintor Conv Block\n",
        "        def dis_initial_conv_block(curr_dim,K=4,S=2,P=1,bias_=False):\n",
        "          return nn.Sequential(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=K, stride=S, padding=P,bias=False),\n",
        "                               nn.LeakyReLU(0.01))\n",
        "          \n",
        "        def dis_mid_conv_block(curr_dim,K=4,S=2,P=1,bias_=False):\n",
        "          return nn.Sequential(nn.Conv2d(curr_dim+3, curr_dim, kernel_size=3, stride=1, padding=1,bias=False), nn.LeakyReLU(0.01),\n",
        "                               nn.Conv2d(curr_dim, curr_dim*2, kernel_size=K, stride=S, padding=P,bias=False),\n",
        "                               nn.LeakyReLU(0.01))\n",
        "        #def dis_mid_conv_block(curr_dim,K=4,S=2,P=1,bias_=False):\n",
        "          #return nn.Sequential(nn.Conv2d(curr_dim+3, curr_dim*2, kernel_size=K, stride=S, padding=P,bias=False),\n",
        "                               #nn.LeakyReLU(0.01))\n",
        "          \n",
        "        def dis_conv_block_without_rgb(curr_dim,K=4,S=2,P=1,bias_=False):\n",
        "          return nn.Sequential(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=K, stride=S, padding=P,bias=False),\n",
        "                               nn.LeakyReLU(0.01))\n",
        "\n",
        "        # Initiate discrimator blocks module list\n",
        "        self.Dis_blocks = ModuleList()\n",
        "\n",
        "        # Fill the module list of both discriminator blocks and rgb converter\n",
        "        for i in range(1, repeat_num):\n",
        "          if i == 1:\n",
        "            self.Dis_blocks.append(dis_initial_conv_block(curr_dim,4,2,1,False))\n",
        "            from_rgb_ = from_rgb(int(curr_dim))\n",
        "            self.rgb_to_features.append(from_rgb_)            \n",
        "            curr_dim = curr_dim * 2\n",
        "          elif i < (repeat_num-2) and (i != 1):\n",
        "            self.Dis_blocks.append(dis_mid_conv_block(curr_dim,4,2,1,False))\n",
        "            from_rgb_ = from_rgb(int(curr_dim))\n",
        "            self.rgb_to_features.append(from_rgb_)            \n",
        "            curr_dim = curr_dim * 2\n",
        "          elif i == repeat_num:\n",
        "            self.Dis_blocks.append(dis_conv_block_without_rgb(curr_dim,4,2,1,False))\n",
        "          else:\n",
        "            self.Dis_blocks.append(dis_conv_block_without_rgb(curr_dim,4,2,1,False))      \n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        kernel_size = int(image_size / np.power(2, repeat_num)) # this code was modified to fit kernel 8x8 to get the class\n",
        "        print(\"kernel size\",kernel_size)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        if image_size == 1024:\n",
        "          self.conv2=nn.Sequential(nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, stride=1, padding=0,bias=False),\n",
        "                                   nn.LeakyReLU(0.01),nn.Conv2d(c_dim, c_dim, kernel_size=kernel_size-1, stride=1, padding=0,bias=False),\n",
        "                                   nn.LeakyReLU(0.01),nn.Conv2d(c_dim, c_dim, kernel_size=kernel_size-1, stride=1, padding=0,bias=False), nn.LeakyReLU(0.01))\n",
        "        elif image_size == 512:\n",
        "          self.conv2=nn.Sequential(nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size+2, stride=1, padding=0,bias=False),\n",
        "                                   nn.LeakyReLU(0.01)) # doesn't work with 512\n",
        "        \n",
        "        elif image_size == 256:\n",
        "          self.conv2=nn.Sequential(nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size+1, stride=1, padding=0,bias=False),\n",
        "                                   nn.LeakyReLU(0.01)) # doesn't work with 256\n",
        "\n",
        "        \n",
        "        \n",
        "    def forward(self, inputs):\n",
        "\n",
        "        # First Input\n",
        "        y = self.rgb_to_features[0](inputs[0])\n",
        "        #print(\"y rgb to features\",y.size())\n",
        "        y = self.Dis_blocks[0](y)\n",
        "        #print(\"dis_block\",y.size())\n",
        "\n",
        "        #counter=0\n",
        "          \n",
        "        for x, block in \\\n",
        "                zip((inputs[1:]),\n",
        "                    (self.Dis_blocks[1:])):\n",
        "            \n",
        "            #def get_shapes(Depth):\n",
        "            #  return [inputs[i].size() for i in range(Depth)]\n",
        "            #print('full input shape:', get_shapes(5) )\n",
        "\n",
        "            #print('x: ', x.size())\n",
        "            #input_part = converter(x)  # convert the input:\n",
        "            #print(\"input_part: \", input_part.size() )\n",
        "            #print(converter)\n",
        "            y = torch.cat((x, y), dim=1)  # concatenate the inputs:\n",
        "            #print(\"concat input_part(here x instead) and y:\",y.size())\n",
        "            #print('block:', block)\n",
        "            y = block(y)  # apply the block\n",
        "            #print(\"y block(y):\",y.size())\n",
        "            #counter=counter+1\n",
        "            #print(counter)\n",
        "\n",
        "        y = self.Dis_blocks[-2](y)\n",
        "        #print(y.size())\n",
        "        y = self.Dis_blocks[-1](y)\n",
        "        #print(\"y last\",y.size())\n",
        "        # calculate the final block:\n",
        "        #input_part = self.final_converter(inputs[0])\n",
        "        #y = th.cat((input_part, y), dim=1)\n",
        "        #y = self.final_block(y)\n",
        "\n",
        "        out_src = self.conv1(y)\n",
        "        #print(\"out_src: \",out_src.size())\n",
        "        out_cls = self.conv2(y)\n",
        "        #print(\"out_cls: \",out_cls.size())\n",
        "        return out_src, out_cls.view(out_cls.size(0), out_cls.size(1))\n",
        "\"\"\"\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  # discriminator takes 9 scaled images\n",
        "  model=Discriminator(ngpu=0, conv_dim=16, c_dim=1, repeat_num=8)\n",
        "  print(model)\n",
        "\n",
        "  num_params = 0\n",
        "  for p in model.parameters():\n",
        "      # original\n",
        "      num_params += p.numel()\n",
        "      #num_params = num_params + p.numel()\n",
        "  print(\"The number of parameters: {}\".format(num_params))  \n",
        "\n",
        "  batch_size = 3\n",
        "  images = torch.randn(batch_size, 3, 1024, 1024)\n",
        "  #data = torch.randn(batch_size, 1)\n",
        "  #output = model(image)\n",
        "\n",
        "\n",
        "  # create images from different scale using avg_pool2d\n",
        "  from torch.nn.functional import avg_pool2d\n",
        "  depth=5\n",
        "  images = [images] + [avg_pool2d(images, int(np.power(2, i))) for i in range(1, depth)]\n",
        "\n",
        "  #images = list(reversed(images))\n",
        "\n",
        "  def get_shapes(Depth):\n",
        "    return [images[i].size() for i in range(Depth)]\n",
        "  print('images shape:', get_shapes(depth) )\n",
        "\n",
        "  #images = list(reversed(images))\n",
        "  #print('images shape reversed:', get_shapes(depth) )\n",
        "\n",
        "\n",
        "  output = model(images)\n",
        "\n",
        "  # Generator takes batch of images only\n",
        "  model=Generator(ngpu=0, conv_dim=16, c_dim=1, repeat_num=6)\n",
        "  print(model)\n",
        "\n",
        "  num_params = 0\n",
        "  for p in model.parameters():\n",
        "      # original\n",
        "      num_params += p.numel()\n",
        "      #num_params = num_params + p.numel()\n",
        "  print(\"The number of parameters: {}\".format(num_params))\n",
        "  \n",
        "  # generate random noisy images\n",
        "  batch_size = 3  \n",
        "  image = torch.randn(batch_size, 3, 1024, 1024)\n",
        "  data = torch.randn(batch_size, 1)\n",
        "  output = model(image,data)\n",
        "  def get_shapes(Depth):\n",
        "    return [output[i].size() for i in range(Depth)]\n",
        "  print('output shape:', get_shapes(5) )\n",
        "\"\"\"\n",
        "  \n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\"\"\"Residual Block with instance normalization.\"\"\"\"\"\"\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True),\n",
        "            nn.ReLU(inplace=True), # nn.ReLU(inplace=True) originally\n",
        "            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.main(x)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\"\"\"Generator network.\"\"\"\"\"\"\n",
        "    def __init__(self, ngpu=0, conv_dim=64, c_dim=5, repeat_num=6):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = torch.cuda.device_count()\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(3+c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False))\n",
        "        layers.append(nn.InstanceNorm2d(conv_dim, affine=True, track_running_stats=True))\n",
        "        layers.append(nn.ReLU(inplace=True)) # before it was True\n",
        "\n",
        "        # Down-sampling layers.\n",
        "        curr_dim = conv_dim\n",
        "        for i in range(2):\n",
        "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False))\n",
        "            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True, track_running_stats=True))\n",
        "            layers.append(nn.ReLU(inplace=True)) # before it was True\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        # Bottleneck layers.\n",
        "        for i in range(repeat_num):\n",
        "            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))\n",
        "\n",
        "        # Up-sampling layers.\n",
        "        for i in range(2):\n",
        "            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=4, stride=2, padding=1, bias=False))\n",
        "            layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True, track_running_stats=True))\n",
        "            layers.append(nn.ReLU(inplace=True)) # was True\n",
        "            curr_dim = curr_dim // 2\n",
        "\n",
        "        layers.append(nn.Conv2d(curr_dim, 3, kernel_size=7, stride=1, padding=3, bias=False))\n",
        "        self.main = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        # Replicate spatially and concatenate domain information.\n",
        "        c = c.view(c.size(0), c.size(1), 1, 1)\n",
        "        c = c.repeat(1, 1, x.size(2), x.size(3))\n",
        "        x = torch.cat([x, c], dim=1)\n",
        "        return self.main(x)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\"\"\"Discriminator network with PatchGAN.\"\"\"\"\"\"\n",
        "    def __init__(self, ngpu=0, image_size=128, conv_dim=64, c_dim=5, repeat_num=6):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu=torch.cuda.device_count()\n",
        "        layers = []\n",
        "        layers.append(nn.Conv2d(3, conv_dim, kernel_size=4, stride=2, padding=1))\n",
        "        layers.append(nn.LeakyReLU(0.01))\n",
        "         \n",
        "\n",
        "        curr_dim = conv_dim\n",
        "        for i in range(1, repeat_num):\n",
        "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1))\n",
        "            layers.append(nn.LeakyReLU(0.01))\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        kernel_size = int(image_size / np.power(2, repeat_num))\n",
        "        self.main = nn.Sequential(*layers)\n",
        "        self.conv1 = nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, bias=False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = self.main(x)\n",
        "        out_src = self.conv1(h)\n",
        "        out_cls = self.conv2(h)\n",
        "        # h1 clone removed..probably not needed\n",
        "        return out_src, out_cls.view(out_cls.size(0), out_cls.size(1))\n",
        "\"\"\"\n",
        "\n",
        "# logger\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class Logger(object):\n",
        "    \"\"\"Tensorboard logger.\"\"\"\n",
        "\n",
        "    def __init__(self, log_dir):\n",
        "        \"\"\"Initialize summary writer.\"\"\"\n",
        "        # self.writer = tf.summary.FileWriter(log_dir) # depreciated\n",
        "        # self.writer = tf.compat.v1.summary.FileWriter(log_dir) # new working one\n",
        "        \n",
        "        #self.writer = tf.contrib.summary.FileWriter(log_dir)\n",
        "        #self.writer = tf.train.SummaryWriter(log_dir)\n",
        "        \n",
        "        # for version 2.xx\n",
        "        self.writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "    def scalar_summary(self, tag, value, step):\n",
        "        \"\"\"Add scalar summary.\"\"\"\n",
        "        #summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
        "        #self.writer.add_summary(summary, step)\n",
        "        \n",
        "        # tf version 2.x\n",
        "        with self.writer.as_default():\n",
        "            tf.summary.scalar(tag, value, step=step)\n",
        "            self.writer.flush()\n",
        "\n",
        "            \n",
        "# brats_auc\n",
        "\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "imgs = glob(\"brats_syn_256_lambda0.1/results/*.jpg\")\n",
        "gts = [0]*975 + [1]*486\n",
        "\n",
        "preds = []\n",
        "\n",
        "for i in range(len(imgs)):\n",
        "    im = np.array(Image.open(\"brats_syn_256_lambda0.1/results/{}-images.jpg\".format(i+1)))\n",
        "    rows = np.split(im, im.shape[0]//256, axis=0)\n",
        "\n",
        "    for r in rows:\n",
        "        cols = np.split(r, 5, axis=1)\n",
        "        preds.append(np.max( np.mean(cols[1], axis=-1) ))\n",
        "\n",
        "#print(roc_auc_score(gts, preds))\n",
        "\n",
        "            \n",
        "# Solver\n",
        "\n",
        "#from model import Generator\n",
        "#from model import Discriminator\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "from sklearn.cluster import KMeans\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch.nn.parallel import DataParallel\n",
        "\n",
        "\n",
        "class Solver(object):\n",
        "    \"\"\"Solver for training and testing Fixed-Point GAN.\"\"\"\n",
        "\n",
        "    def __init__(self, data_loader, config):\n",
        "        \"\"\"Initialize configurations.\"\"\"\n",
        "\n",
        "        # Data loader.\n",
        "        self.data_loader = data_loader\n",
        "\n",
        "        # Model configurations.\n",
        "        self.c_dim = config.c_dim\n",
        "        self.c2_dim = config.c2_dim\n",
        "        self.image_size = config.image_size\n",
        "        self.g_conv_dim = config.g_conv_dim\n",
        "        self.d_conv_dim = config.d_conv_dim\n",
        "        self.g_repeat_num = config.g_repeat_num\n",
        "        self.d_repeat_num = config.d_repeat_num\n",
        "        self.lambda_cls = config.lambda_cls\n",
        "        self.lambda_rec = config.lambda_rec\n",
        "        self.lambda_gp = config.lambda_gp\n",
        "        self.lambda_id = config.lambda_id\n",
        "        self.depth = config.depth\n",
        "\n",
        "        # Training configurations.\n",
        "        self.dataset = config.dataset\n",
        "        self.batch_size = config.batch_size\n",
        "        self.num_iters = config.num_iters\n",
        "        self.num_iters_decay = config.num_iters_decay\n",
        "        self.g_lr = config.g_lr\n",
        "        self.d_lr = config.d_lr\n",
        "        self.n_critic = config.n_critic\n",
        "        self.beta1 = config.beta1\n",
        "        self.beta2 = config.beta2\n",
        "        self.resume_iters = config.resume_iters\n",
        "        self.selected_attrs = config.selected_attrs\n",
        "\n",
        "        # Test configurations.\n",
        "        self.test_iters = config.test_iters\n",
        "\n",
        "        # Miscellaneous.\n",
        "        self.use_tensorboard = config.use_tensorboard\n",
        "        #self.ngpu= config.ngpu\n",
        "        self.ngpu= torch.cuda.device_count()\n",
        "        self.device = torch.device('cuda' if (torch.cuda.is_available() and self.ngpu>0) else 'cpu')\n",
        "        print('Number of Cuda devices: ',torch.cuda.device_count())\n",
        "\n",
        "        # Directories.\n",
        "        self.log_dir = config.log_dir\n",
        "        self.sample_dir = config.sample_dir\n",
        "        self.model_save_dir = config.model_save_dir\n",
        "        self.result_dir = config.result_dir\n",
        "\n",
        "        # Step size.\n",
        "        self.log_step = config.log_step\n",
        "        self.sample_step = config.sample_step\n",
        "        self.model_save_step = config.model_save_step\n",
        "        self.lr_update_step = config.lr_update_step\n",
        "\n",
        "        self.rank=0\n",
        "        self.world_size=1\n",
        "\n",
        "        # Build the model and tensorboard.\n",
        "        self.build_model()\n",
        "        if self.use_tensorboard:\n",
        "          self.build_tensorboard()\n",
        "    \n",
        "    def setup(self,rank,world_size):\n",
        "        os.environ['MASTER_ADDR'] = 'localhost'\n",
        "        os.environ['MASTER_PORT'] = '12355'\n",
        "        # initialize the process group\n",
        "        dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Create a generator and a discriminator.\"\"\"\n",
        "\n",
        "        if self.dataset in ['CelebA', 'BRATS', 'Directory']:\n",
        "            self.G = Generator(self.ngpu,self.g_conv_dim, self.c_dim, self.g_repeat_num,self.depth)\n",
        "            self.D = Discriminator(self.ngpu,self.image_size, self.d_conv_dim, self.c_dim, self.d_repeat_num,self.depth) \n",
        "\n",
        "        #self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.g_lr, [self.beta1, self.beta2])\n",
        "        #self.d_optimizer = torch.optim.Adam(self.D.parameters(), self.d_lr, [self.beta1, self.beta2])\n",
        "        self.print_network(self.G, 'Generator')\n",
        "        self.print_network(self.D, 'Discriminator')\n",
        "    \n",
        "        self.G.to(self.device)\n",
        "        self.D.to(self.device)\n",
        "\n",
        "        # Handle multi-gpu if desired\n",
        "        if (self.device.type == 'cuda') and (self.ngpu > 1):\n",
        "          # added for parallelism\n",
        "          #for i in list(range(self.ngpu)):\n",
        "          #  torch.cuda.set_device(i)       \n",
        "          #  self.setup(self.rank,self.world_size)\n",
        "          #  self.G = DistributedDataParallel(self.G, device_ids=[i],output_device=i)\n",
        "          #  self.D = DistributedDataParallel(self.D, device_ids=[i],output_device=i)\n",
        "\n",
        "          #self.setup(self.rank,self.world_size)\n",
        "          #self.G = DistributedDataParallel(self.G, device_ids=list(range(self.ngpu)))\n",
        "          #self.D = DistributedDataParallel(self.D, device_ids=list(range(self.ngpu)))\n",
        "          \n",
        "          self.G = DataParallel(self.G, device_ids=list(range(self.ngpu)))\n",
        "          self.D = DataParallel(self.D, device_ids=list(range(self.ngpu)))\n",
        "\n",
        "          self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.g_lr, [self.beta1, self.beta2])\n",
        "          self.d_optimizer = torch.optim.Adam(self.D.parameters(), self.d_lr, [self.beta1, self.beta2])\n",
        "\n",
        "        else:\n",
        "\n",
        "          self.g_optimizer = torch.optim.Adam(self.G.parameters(), self.g_lr, [self.beta1, self.beta2])\n",
        "          self.d_optimizer = torch.optim.Adam(self.D.parameters(), self.d_lr, [self.beta1, self.beta2])\n",
        "\n",
        "    def recreate_image(self, codebook, labels, w, h):\n",
        "        \"\"\"Recreate the (compressed) image from the code book & labels\"\"\"\n",
        "        d = codebook.shape[1]\n",
        "        image = np.zeros((w, h, d))\n",
        "        label_idx = 0\n",
        "        for i in range(w):\n",
        "            for j in range(h):\n",
        "                image[i][j] = codebook[labels[label_idx]]\n",
        "                # orginal\n",
        "                # label_idx += 1\n",
        "                label_idx = label_idx + 1\n",
        "        return image\n",
        "\n",
        "    def print_network(self, model, name):\n",
        "        \"\"\"Print out the network information.\"\"\"\n",
        "        num_params = 0\n",
        "        for p in model.parameters():\n",
        "            # original\n",
        "            # num_params += p.numel()\n",
        "            num_params = num_params + p.numel()\n",
        "        print(model)\n",
        "        print(name)\n",
        "        print(\"The number of parameters: {}\".format(num_params))\n",
        "\n",
        "    def restore_model(self, resume_iters):\n",
        "        \"\"\"Restore the trained generator and discriminator.\"\"\"\n",
        "        print('Loading the trained models from step {}...'.format(resume_iters))\n",
        "        G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(resume_iters))\n",
        "        D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(resume_iters))\n",
        "        self.G.load_state_dict(torch.load(G_path, map_location=lambda storage, loc: storage)) # Load all tensors onto the CPU, using a function\n",
        "        self.D.load_state_dict(torch.load(D_path, map_location=lambda storage, loc: storage)) # Load all tensors onto the CPU, using a function # to avoid GPU RAM surge when loading a model checkpoint.\n",
        "\n",
        "    def build_tensorboard(self):\n",
        "        \"\"\"Build a tensorboard logger.\"\"\"\n",
        "        #from logger import Logger\n",
        "        self.logger = Logger(self.log_dir)\n",
        "\n",
        "    def update_lr(self, g_lr, d_lr):\n",
        "        \"\"\"Decay learning rates of the generator and discriminator.\"\"\"\n",
        "        for param_group in self.g_optimizer.param_groups:\n",
        "            param_group['lr'] = g_lr\n",
        "        for param_group in self.d_optimizer.param_groups:\n",
        "            param_group['lr'] = d_lr\n",
        "\n",
        "    def reset_grad(self):\n",
        "        \"\"\"Reset the gradient buffers.\"\"\"\n",
        "        self.g_optimizer.zero_grad()\n",
        "        self.d_optimizer.zero_grad()\n",
        "\n",
        "    def denorm(self, x):\n",
        "        \"\"\"Convert the range from [-1, 1] to [0, 1].\"\"\"\n",
        "        out = (x + 1) / 2\n",
        "        return out.clamp_(0, 1)\n",
        "\n",
        "    def gradient_penalty(self, y, x):\n",
        "        \"\"\"Compute gradient penalty: (L2_norm(dy/dx) - 1)**2.\"\"\"\n",
        "        weight = torch.ones(y.size()).to(self.device)\n",
        "        dydx = torch.autograd.grad(outputs=y,\n",
        "                                   inputs=x,\n",
        "                                   grad_outputs=weight,\n",
        "                                   retain_graph=True,\n",
        "                                   create_graph=True,\n",
        "                                   only_inputs=True)[0]\n",
        "\n",
        "        dydx = dydx.view(dydx.size(0), -1)\n",
        "        dydx_l2norm = torch.sqrt(torch.sum(dydx**2, dim=1))\n",
        "        return torch.mean((dydx_l2norm-1)**2)\n",
        "\n",
        "    def label2onehot(self, labels, dim):\n",
        "        \"\"\"Convert label indices to one-hot vectors.\"\"\"\n",
        "        batch_size = labels.size(0)\n",
        "        out = torch.zeros(batch_size, dim)\n",
        "        out[np.arange(batch_size), labels.long()] = 1\n",
        "        return out\n",
        "\n",
        "    def create_labels(self, c_org, c_dim=5, dataset='CelebA', selected_attrs=None):\n",
        "        \"\"\"Generate target domain labels for debugging and testing.\"\"\"\n",
        "        # Get hair color indices.\n",
        "        if dataset in ['CelebA']:\n",
        "            hair_color_indices = []\n",
        "            for i, attr_name in enumerate(selected_attrs):\n",
        "                if attr_name in ['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair']:\n",
        "                    hair_color_indices.append(i)\n",
        "\n",
        "        c_trg_list = []\n",
        "        for i in range(c_dim):\n",
        "            if dataset in ['CelebA']:\n",
        "                c_trg = c_org.clone()\n",
        "                if i in hair_color_indices:  # Set one hair color to 1 and the rest to 0.\n",
        "                    c_trg[:, i] = 1\n",
        "                    for j in hair_color_indices:\n",
        "                        if j != i:\n",
        "                            c_trg[:, j] = 0\n",
        "                else:\n",
        "                    c_trg[:, i] = (c_trg[:, i] == 0)  # Reverse attribute value.\n",
        "            elif dataset == 'BRATS':\n",
        "                c_trg = c_org.clone()\n",
        "                c_trg[:, i] = (c_trg[:, i] == 0)  # Reverse attribute value.\n",
        "            elif dataset == 'Directory':\n",
        "                c_trg = self.label2onehot(torch.ones(c_org.size(0))*i, c_dim)\n",
        "\n",
        "            c_trg_list.append(c_trg.to(self.device))\n",
        "        return c_trg_list\n",
        "\n",
        "    def classification_loss(self, logit, target, dataset='CelebA'):\n",
        "        \"\"\"Compute binary or softmax cross entropy loss.\"\"\"\n",
        "        if dataset in ['CelebA', 'BRATS']:\n",
        "            return F.binary_cross_entropy_with_logits(logit, target, reduction='sum') / logit.size(0)  # size_average=False inside binary_cross_entropy_with_logits\n",
        "        elif dataset == 'Directory':\n",
        "            return F.cross_entropy(logit, target)\n",
        "\n",
        "    def create_grid(self, samples, img_files):\n",
        "        \"\"\"\n",
        "        utility function to create a grid of GAN samples\n",
        "        :param samples: generated samples for storing list[Tensors]\n",
        "        :param img_files: list of names of files to write\n",
        "        :return: None (saves multiple files)\n",
        "        \"\"\"\n",
        "        from torchvision.utils import save_image\n",
        "        from torch.nn.functional import interpolate\n",
        "        from numpy import sqrt, power\n",
        "\n",
        "        # dynamically adjust the colour of the images\n",
        "        samples = [Generator.adjust_dynamic_range(sample) for sample in samples]\n",
        "\n",
        "        # resize the samples to have same resolution:\n",
        "        depth__=5 # quick fix\n",
        "        for i in range(len(samples)):\n",
        "            samples[i] = interpolate(samples[i],\n",
        "                                     scale_factor=power(2,\n",
        "                                                        depth__ - 1 - i))\n",
        "        # save the images:\n",
        "        for sample, img_file in zip(samples, img_files):\n",
        "            save_image(sample, img_file, nrow=int(sqrt(sample.shape[0])),\n",
        "                       normalize=True, scale_each=True, padding=0) \n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train Fixed-Point GAN within a single dataset.\"\"\"\n",
        "        # Set data loader.\n",
        "        if self.dataset in ['CelebA', 'BRATS', 'Directory']:\n",
        "            data_loader = self.data_loader\n",
        "\n",
        "        # Fetch fixed inputs for debugging.\n",
        "        data_iter = iter(data_loader)\n",
        "        x_fixed, c_org = next(data_iter)\n",
        "        x_fixed = x_fixed.to(self.device)\n",
        "        c_fixed_list = self.create_labels(c_org, self.c_dim, self.dataset, self.selected_attrs)\n",
        "\n",
        "        # Learning rate cache for decaying.\n",
        "        g_lr = self.g_lr\n",
        "        d_lr = self.d_lr\n",
        "\n",
        "        # Start training from scratch or resume training.\n",
        "        start_iters = 0\n",
        "        if self.resume_iters:\n",
        "            start_iters = self.resume_iters\n",
        "            self.restore_model(self.resume_iters)\n",
        "\n",
        "        # Start training.\n",
        "        print('Start training...')\n",
        "        start_time = time.time()\n",
        "        for i in range(start_iters, self.num_iters):\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                             1. Preprocess input data                                #\n",
        "            # =================================================================================== #\n",
        "            \n",
        "\n",
        "            # Fetch real images and labels.\n",
        "            try:\n",
        "                x_real, label_org = next(data_iter)\n",
        "                #print(\"x_real\",x_real.size()) # added to test the dimensions\n",
        "                #print(\"label_org\",label_org.size()) # added to test the dimensions\n",
        "            except:\n",
        "                data_iter = iter(data_loader)\n",
        "                x_real, label_org = next(data_iter)\n",
        "\n",
        "                #print(\"x_real: \",x_real.size()) # added to test the dimensions\n",
        "                #print(\"label_org: \",label_org.size()) # added to test the dimensions\n",
        "\n",
        "            # Generate target domain labels randomly.\n",
        "            rand_idx = torch.randperm(label_org.size(0))\n",
        "            label_trg = label_org[rand_idx]\n",
        "\n",
        "            #print(\"rand_idx\",rand_idx.size()) # added to test the dimensions\n",
        "            #print(\"label_trg\",label_trg.size()) # added to test the dimensions\n",
        "\n",
        "            if self.dataset in ['CelebA', 'BRATS']:\n",
        "                c_org = label_org.clone()\n",
        "                c_trg = label_trg.clone()\n",
        "            elif self.dataset == 'Directory':\n",
        "                c_org = self.label2onehot(label_org, self.c_dim)\n",
        "                c_trg = self.label2onehot(label_trg, self.c_dim)\n",
        "            \n",
        "            # added to output everything on 1 gpu \n",
        "            if (self.device.type == 'cuda') and (self.ngpu > 1):\n",
        "              torch.cuda.set_device(0)   \n",
        "            \n",
        "            x_real = x_real.to(self.device)           # Input images.\n",
        "            c_org = c_org.to(self.device)             # Original domain labels.\n",
        "            c_trg = c_trg.to(self.device)             # Target domain labels.\n",
        "            label_org = label_org.to(self.device)     # Labels for computing classification loss.\n",
        "            label_trg = label_trg.to(self.device)     # Labels for computing classification loss.\n",
        "\n",
        "            #print(\"c_org\",c_org.size()) # added to test the dimensions\n",
        "            #print(\"c_targ\",c_trg.size()) # added to test the dimensions\n",
        "            \n",
        "\n",
        "            # testing batch_size and image pyramid\n",
        "            from torch.nn.functional import avg_pool2d\n",
        "            batch_images=x_real  \n",
        "            depth__=5\n",
        "            batch_images = [batch_images] + [avg_pool2d(batch_images, int(np.power(2, i)))\n",
        "                                     for i in range(1, depth__)]\n",
        "                            \n",
        "            #print('image length list: ',len(batch_images)) # added : equal number of depth or layers\n",
        "            #def get_shapes(Depth):\n",
        "            #  return [batch_images[i].size() for i in range(Depth)]\n",
        "            #print('images shape:', get_shapes(depth__) )\n",
        "\n",
        "\n",
        "            # list needs to be added to a torch to be able to be used with to(self.device)\n",
        "            #x_real=[_x_real.to(self.device) for _x_real in batch_images]    # input images\n",
        "            #data = [_data.cuda() for _data in data] \n",
        "            #c_org = c_org.repeat(5,1,1).to(self.device)           # Original domain labels.\n",
        "            #c_trg = c_trg.repeat(5,1,1).to(self.device)         # Target domain labels.\n",
        "            #label_org = label_org.repeat(5,1,1).to(self.device)     # Labels for computing classification loss.\n",
        "            #label_trg = label_trg.repeat(5,1,1).to(self.device)     # Labels for computing classification loss.\n",
        "\n",
        "            #for _x_real in batch_images: \n",
        "            \"\"\"\n",
        "            def padding(ListofTensorImages):\n",
        "              out=[]\n",
        "              for img in ListofTensorImages:\n",
        "                print(img.size())\n",
        "                equ= (1024 - img.size(-1))/2\n",
        "                print(equ)\n",
        "                m = nn.ZeroPad2d(int(equ))\n",
        "                out.append(m(img))\n",
        "              return out\n",
        "\n",
        "            ImagesWithPadding=padding(batch_images)\n",
        "            for i in range(len(ImagesWithPadding)):\n",
        "              print(ImagesWithPadding[i].size())\n",
        "\n",
        "            stackedImages= torch.stack([ImagesWithPadding[0],ImagesWithPadding[1],ImagesWithPadding[2],ImagesWithPadding[3],ImagesWithPadding[4]],dim=0)\n",
        "            \"\"\"\n",
        "\n",
        "            x_real_d=batch_images #.to(self.device)\n",
        "            c_org = c_org.to(self.device)           # Original domain labels.\n",
        "            c_trg = c_trg.to(self.device)         # Target domain labels.\n",
        "            label_org = label_org.to(self.device)     # Labels for computing classification loss.\n",
        "            label_trg = label_trg.to(self.device)     # Labels for computing classification loss.\n",
        "\n",
        "            #def get_shapes_01(Depth):\n",
        "            #  return [batch_images[i].size() for i in range(Depth)],[c_org[i].size() for i in range(Depth)],[c_trg[i].size() for i in range(Depth)],[label_org[i].size() for i in range(Depth)],[label_trg[i].size() for i in range(Depth)]\n",
        "            #print('batch_images, c_org, c_trig , label_org , label_trg :', get_shapes_01(depth__) )  \n",
        "\n",
        "   \n",
        "            # =================================================================================== #\n",
        "            #                             2. Train the discriminator                              #\n",
        "            # =================================================================================== #\n",
        "\n",
        "\n",
        "          #  # Compute loss with real images.\n",
        "            \n",
        "            t0=time.time()\n",
        "            out_src, out_cls = self.D(x_real_d)            \n",
        "            d_loss_real = - torch.mean(out_src)\n",
        "            d_loss_cls = self.classification_loss(out_cls, label_org, self.dataset)\n",
        "            #print(\"Loss with real images\",time.time()-t0)\n",
        "\n",
        "            #print(\"out_src\",out_src.size()) # added to test the dimensions\n",
        "            #print(\"out_cls\",out_cls.size()) # added to test the dimensions\n",
        "            #print(\"d_loss_real\",d_loss_real.size()) # added to test the dimensions\n",
        "            #print(\"d_loss_cls\",d_loss_cls.size()) # added to test the dimensions\n",
        "          \n",
        "          # Compute loss with fake images.\n",
        "            t0=time.time()\n",
        "            delta = self.G(x_real, c_trg) # c_trg is target class  # output 5 images : output the difference   \n",
        "            \n",
        "            #def get_shapes(Depth):\n",
        "            #  return [delta[i].size() for i in range(Depth)]\n",
        "            #print('images shape delta, results of Generator:', get_shapes(depth__) )\n",
        "            #print(\"delta last batch\",delta[-1].size())\n",
        "\n",
        "            delta=delta[::-1] # inverse List\n",
        "            x_fake=[torch.tanh(x_real_d[i] + delta[i]) for i in range(len(x_real_d))] # of 1 image 4x3x1024x1024\n",
        "            x_fake=[x_fake[i].detach() for i in range(len(x_real_d))]\n",
        "            out_src, out_cls = self.D(x_fake)\n",
        "            d_loss_fake = torch.mean(out_src)\n",
        "            #print(\"Loss with fake images\",time.time()-t0)\n",
        "            \n",
        "            #print(\"out_src\",out_src.size()) # added to test the dimensions\n",
        "            #print(\"out_cls\",out_cls.size()) # added to test the dimensions\n",
        "            #print(\"d_loss_fake\",d_loss_fake.size()) # added to test the dimensions\n",
        "  \n",
        "\n",
        "          # Compute loss for gradient penalty.\n",
        "            t0=time.time()\n",
        "            alpha = torch.rand(x_real.size(0), 1, 1, 1).to(self.device) # x_real.size(0) is the nb of batch\n",
        "            #x_hat = (alpha * x_real.data + (1 - alpha) * x_fake.data).requires_grad_(True) \n",
        "            x_hat = [(alpha * x_real_d[i].data + (1 - alpha) * x_fake[i].data).requires_grad_(True) for i in range(len(x_real_d))]\n",
        "            out_src, _ = self.D(x_hat)\n",
        "            d_loss_gp = self.gradient_penalty(out_src, x_hat)\n",
        "            #print(\"Loss for gredient penalty\",time.time()-t0)\n",
        "            \n",
        "            #check the gpu before changing\n",
        "            #print(d_loss_gp.get_device())\n",
        "\n",
        "            #print(\"alpha, compute gradient penalty\",alpha) # added to test the dimensions\n",
        "            #print(\"out_src\",out_src.size()) # added to test the dimensions\n",
        "            #print(\"d_loss_gp\",d_loss_gp) # added to test the dimensions\n",
        "    \n",
        "\n",
        "            # Backward and optimize.#\n",
        "            d_loss = d_loss_real + d_loss_fake + self.lambda_cls * d_loss_cls + self.lambda_gp * d_loss_gp \n",
        "\n",
        "            #print(\"d_loss\",d_loss) # added to test the dimensions\n",
        "            \n",
        "            t0=time.time()\n",
        "            self.reset_grad()\n",
        "\n",
        "            if (self.device.type == 'cuda') and (self.ngpu > 1):\n",
        "              d_loss.mean().backward()\n",
        "            else:\n",
        "              d_loss.backward()\n",
        "\n",
        "            self.d_optimizer.step()\n",
        "            #print(\"back propagation and optimization\",time.time()-t0)\n",
        "            # Logging.\n",
        "            loss = {}\n",
        "            loss['D/loss_real'] = d_loss_real.item()\n",
        "            loss['D/loss_fake'] = d_loss_fake.item()\n",
        "            loss['D/loss_cls'] = d_loss_cls.item()\n",
        "            loss['D/loss_gp'] = d_loss_gp.item()\n",
        "            # added\n",
        "            loss['d_loss'] = d_loss.item()\n",
        "            \n",
        "            # =================================================================================== #\n",
        "            #                               3. Train the generator                                #\n",
        "            # =================================================================================== #\n",
        "            \n",
        "            if (i+1) % self.n_critic == 0:\n",
        "\n",
        "                # Original-to-target domain.\n",
        "                t0=time.time()\n",
        "                delta = self.G(x_real, c_trg)\n",
        "                delta=delta[::-1]\n",
        "                x_fake=[torch.tanh(x_real_d[i] + delta[i]) for i in range(len(x_real_d))] # of 1 image 4x3x1024x1024\n",
        "                #x_fake=[x_fake[i].detach() for i in range(len(x_real_d))]\n",
        "                out_src, out_cls = self.D(x_fake)\n",
        "                #x_fake = torch.tanh(x_real_d + delta)\n",
        "                #out_src, out_cls = self.D(x_fake)\n",
        "                g_loss_fake = - torch.mean(out_src)\n",
        "                g_loss_cls = self.classification_loss(out_cls, label_trg, self.dataset)\n",
        "                \n",
        "                #print(\"Original-to-target domain : Generator\",time.time()-t0)\n",
        "\n",
        "                #print(\"delta, generator\",delta.size()) # added to test the dimensions\n",
        "                #print(\"x_fake: tanh(x_real + delta)\",x_fake.size()) # added to test the dimensions\n",
        "                #print(\"out_src\",out_src.size()) # added to test the dimensions\n",
        "                #print(\"out_cls\",out_cls.size()) # added to test the dimensions\n",
        "                #print(\"g_loss_fake\",g_loss_fake) # added to test the dimensions\n",
        "                #print(\"g_loss_cls\",g_loss_cls) # added to test the dimensions\n",
        "\n",
        "\n",
        "\n",
        "                # Original-to-original domain.\n",
        "                t0=time.time()\n",
        "\n",
        "                delta_id = self.G(x_real, c_org)\n",
        "                delta_id=delta_id[::-1]\n",
        "                #x_fake_id = torch.tanh(x_real_d + delta_id)\n",
        "                x_fake_id=[torch.tanh(x_real_d[i] + delta_id[i]) for i in range(len(x_real_d))]\n",
        "                out_src_id, out_cls_id = self.D(x_fake_id)\n",
        "                g_loss_fake_id = - torch.mean(out_src_id)\n",
        "                g_loss_cls_id = self.classification_loss(out_cls_id, label_org, self.dataset)\n",
        "                #g_loss_id = torch.mean(torch.abs(x_real - torch.tanh(delta_id + x_real)))\n",
        "\n",
        "                \n",
        "                def image_padding(ListofTensorImages):\n",
        "                  out=[]\n",
        "                  for img in ListofTensorImages:\n",
        "                    #print(img.size())\n",
        "                    equ= (1024 - img.size(-1))/2\n",
        "                    #print(equ)\n",
        "                    m = nn.ZeroPad2d(int(equ))\n",
        "                    out.append(m(img))\n",
        "                  return out\n",
        "\n",
        "                g_loss_id = torch.mean(torch.stack(image_padding([torch.abs(x_real_d[i] - x_fake_id[i]) for i in range(len(x_real_d))])))\n",
        "                #print(g_loss_id)\n",
        "                #print(\"Original-to-original domain : Generator\",time.time()-t0)\n",
        "\n",
        "                #print(\"delta_id, generator\",delta_id.size()) # added to test the dimensions\n",
        "                #print(\"x_fake_id: tanh(x_real + delta_id)\",x_fake_id.size()) # added to test the dimensions\n",
        "                #print(\"out_src_id\",out_src_id.size()) # added to test the dimensions\n",
        "                #print(\"out_cls_id\",out_cls_id.size()) # added to test the dimensions\n",
        "                #print(\"g_loss_fake_id\",g_loss_fake_id) # added to test the dimensions\n",
        "                #print(\"g_loss_cls_id\",g_loss_cls_id) # added to test the dimensions\n",
        "                #print(\"g_loss_id\",g_loss_id) # added to test the dimensions\n",
        "\n",
        "\n",
        "                # Target-to-original domain.\n",
        "                t0=time.time()\n",
        "\n",
        "                #print(\"x_fake[-1] size: \",x_fake[0].size()) # added to test the dimensions\n",
        "                delta_reconst = self.G(x_fake[0], c_org)\n",
        "                delta_reconst=delta_reconst[::-1]\n",
        "                #print(\"delta_reconst[-1] size: \",delta_reconst[-1].size()) # added to test the dimensions\n",
        "\n",
        "\n",
        "                x_reconst=[torch.tanh(x_fake[i] + delta_reconst[i]) for i in range(len(x_real_d))]\n",
        "                #x_reconst = torch.tanh(x_fake + delta_reconst)\n",
        "                #g_loss_rec = torch.mean(torch.abs(x_real - x_reconst))\n",
        "                g_loss_rec= torch.mean(torch.stack(image_padding([torch.abs(x_real_d[i] - x_reconst[i]) for i in range(len(x_real_d))])))\n",
        "\n",
        "                #print(\"delta_reconst, generator\",delta_reconst.size()) # added to test the dimensions\n",
        "                #print(\"x_reconst: tanh(x_real + delta_reconst)\",x_reconst.size()) # added to test the dimensions\n",
        "                #print(\"g_loss_rec\",g_loss_rec) # added to test the dimensions\n",
        "\n",
        "                #print(\"Target-to-original domain : Generator\",time.time()-t0)\n",
        "\n",
        "                # Original-to-original domain.\n",
        "                t0=time.time()\n",
        "\n",
        "                delta_reconst_id = self.G(x_fake_id[0], c_org) \n",
        "                delta_reconst_id = delta_reconst_id[::-1]\n",
        "                x_reconst_id=[torch.tanh(x_fake_id[i] + delta_reconst_id[i]) for i in range(len(x_real_d))]\n",
        "                #x_reconst_id = torch.tanh(x_fake_id + delta_reconst_id)\n",
        "                #g_loss_rec_id = torch.mean(torch.abs(x_real - x_reconst_id))\n",
        "                g_loss_rec_id= torch.mean(torch.stack(image_padding([torch.abs(x_real_d[i] - x_reconst_id[i]) for i in range(len(x_real_d))])))  # 5 scales images \n",
        "\n",
        "\n",
        "                #print(\"delta_reconst_id, generator\",delta_reconst_id.size()) # added to test the dimensions\n",
        "                #print(\"x_reconst_id: tanh(x_real + delta_reconst_id)\",x_reconst_id.size()) # added to test the dimensions\n",
        "                #print(\"g_loss_rec_id\",g_loss_rec_id) # added to test the dimensions\n",
        "\n",
        "                #print(\"Original-to-original domain : Generator\",time.time()-t0)\n",
        "\n",
        "\n",
        "                # Backward and optimize.\n",
        "                g_loss_same = g_loss_fake_id + self.lambda_rec * g_loss_rec_id + self.lambda_cls * g_loss_cls_id + self.lambda_id * g_loss_id\n",
        "                g_loss = g_loss_fake + self.lambda_rec * g_loss_rec + self.lambda_cls * g_loss_cls + g_loss_same\n",
        "\n",
        "                #print(\"g_loss_same\",g_loss_same.size()) # added to test the dimensions\n",
        "                #print(\"g_loss\",g_loss) # added to test the dimensions\n",
        "\n",
        "                t0=time.time()\n",
        "                self.reset_grad()\n",
        "                #g_loss.backward()\n",
        "\n",
        "                if (self.device.type == 'cuda') and (self.ngpu > 1):\n",
        "                  g_loss.mean().backward()\n",
        "                else:\n",
        "                  g_loss.backward()\n",
        "\n",
        "                self.g_optimizer.step()\n",
        "                #print(\"back Propagation: Generator\",time.time()-t0)\n",
        "\n",
        "                # Logging.\n",
        "                loss['G/loss_fake'] = g_loss_fake.item()\n",
        "                loss['G/loss_rec'] = g_loss_rec.item()\n",
        "                loss['G/loss_cls'] = g_loss_cls.item()\n",
        "                loss['G/loss_fake_id'] = g_loss_fake_id.item()\n",
        "                loss['G/loss_rec_id'] = g_loss_rec_id.item()\n",
        "                loss['G/loss_cls_id'] = g_loss_cls_id.item()\n",
        "                loss['G/loss_id'] = g_loss_id.item()\n",
        "                # added\n",
        "                loss['g_loss'] = g_loss.item()\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                                 4. Miscellaneous                                    #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # Print out training information.\n",
        "            if (i+1) % self.log_step == 0:\n",
        "                et = time.time() - start_time\n",
        "                et = str(datetime.timedelta(seconds=et))[:-7]\n",
        "                log = \"Elapsed [{}], Iteration [{}/{}]\".format(et, i+1, self.num_iters)\n",
        "                for tag, value in loss.items():\n",
        "                    log += \", {}: {:.4f}\".format(tag, value)\n",
        "                print(log,flush=True)\n",
        "\n",
        "                if self.use_tensorboard:\n",
        "                    for tag, value in loss.items():\n",
        "                        self.logger.scalar_summary(tag, value, i+1)\n",
        "\n",
        "            # Translate fixed images for debugging.\n",
        "            if (i+1) % self.sample_step == 0:\n",
        "              from torch.nn.functional import avg_pool2d\n",
        "              #batch_images_fixed=x_fixed  \n",
        "              #depth__=5\n",
        "              #batch_images_fixed = [batch_images_fixed] + [avg_pool2d(batch_images_fixed, int(np.power(2, i)))\n",
        "              #                       for i in range(1, depth__)]\n",
        "              with torch.no_grad():\n",
        "                  x_fake_list = [x_fixed]\n",
        "                  print(\"fake_list[0]\",x_fake_list[0].size()) # 3x3x1024x1024   \n",
        "                  for c_fixed in c_fixed_list:\n",
        "                      delta = self.G(x_fixed, c_fixed)\n",
        "                      delta = delta[::-1]\n",
        "                      gen_img=torch.tanh(delta[0] + x_fixed)\n",
        "                      x_fake_list.append(gen_img) # add to list\n",
        "                  x_concat = torch.cat(x_fake_list, dim=3)\n",
        "                  sample_path = os.path.join(self.sample_dir, '{}-images.jpg'.format(i+1))\n",
        "                  save_image(self.denorm(x_concat.data.cpu()), sample_path, nrow=1, padding=0)\n",
        "                  print('Saved real and fake images into {}...'.format(sample_path))\n",
        "\n",
        "\n",
        "                  # create a grid of samples and save it\n",
        "                  #reses = [str(int(np.power(2, dep+4))) + \"_x_\"\n",
        "                  #+ str(int(np.power(2, dep+4)))\n",
        "                  #for dep in reversed(range(2,depth__ + 2))]\n",
        "\n",
        "                  #gen_img_files = [os.path.join(self.sample_dir, res, \"gen_\" +\n",
        "                  #                              str(i) + \".png\")\n",
        "                  #                  for res in reses]\n",
        "\n",
        "                  # Make sure all the required directories exist\n",
        "                  # otherwise make them\n",
        "                  #os.makedirs(self.sample_dir, exist_ok=True)\n",
        "                  #for gen_img_file in gen_img_files:\n",
        "                  #    os.makedirs(os.path.dirname(gen_img_file), exist_ok=True)\n",
        "\n",
        "                  # fake list should contain the x_fake_list of different versions\n",
        "                  #print(\"x_fixed\",x_fixed[0].size()) \n",
        "                  #print(\"delta\",delta[0].size())      \n",
        "                  #gen_imgs=[torch.tanh(batch_images_fixed[i] + delta[i]) for i in range(len(x_real_d))]\n",
        "                  #self.create_grid(gen_imgs,gen_img_files)\n",
        "\n",
        "            # Save model checkpoints.\n",
        "            if (i+1) % self.model_save_step == 0:\n",
        "                G_path = os.path.join(self.model_save_dir, '{}-G.ckpt'.format(i+1))\n",
        "                D_path = os.path.join(self.model_save_dir, '{}-D.ckpt'.format(i+1))\n",
        "                \n",
        "                if (self.device.type == 'cuda') and (self.ngpu > 1):\n",
        "                  torch.save(self.G.module.state_dict(), G_path) # loading doesn't change\n",
        "                  torch.save(self.D.module.state_dict(), D_path) # loading doesn't change\n",
        "                else:\n",
        "                  torch.save(self.G.state_dict(), G_path)\n",
        "                  torch.save(self.D.state_dict(), D_path)\n",
        "\n",
        "                print('Saved model checkpoints into {}...'.format(self.model_save_dir))\n",
        "\n",
        "            # Decay learning rates.\n",
        "            if (i+1) % self.lr_update_step == 0 and (i+1) > (self.num_iters - self.num_iters_decay):\n",
        "                g_lr = g_lr - (self.g_lr / float(self.num_iters_decay))\n",
        "                d_lr = d_lr - (self.d_lr / float(self.num_iters_decay))\n",
        "                #original\n",
        "                #g_lr -= (self.g_lr / float(self.num_iters_decay))\n",
        "                #d_lr -= (self.d_lr / float(self.num_iters_decay))\n",
        "                self.update_lr(g_lr, d_lr)\n",
        "                print ('Decayed learning rates, g_lr: {}, d_lr: {}.'.format(g_lr, d_lr))\n",
        "\n",
        "\n",
        "    def test(self):\n",
        "        \"\"\"Translate images using Fixed-Point GAN trained on a single dataset.\"\"\"\n",
        "        # Load the trained generator.\n",
        "        self.restore_model(self.test_iters)\n",
        "        \n",
        "        # Set data loader.\n",
        "        if self.dataset in ['CelebA', 'Directory']:\n",
        "            data_loader = self.data_loader\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for i, (x_real, c_org) in enumerate(data_loader):\n",
        "\n",
        "                # Prepare input images and target domain labels.\n",
        "                x_real = x_real.to(self.device)\n",
        "                c_trg_list = self.create_labels(c_org, self.c_dim, self.dataset, self.selected_attrs)\n",
        "\n",
        "                # Translate images.\n",
        "                x_fake_list = [x_real]\n",
        "                for c_trg in c_trg_list:\n",
        "                    x_fake_list.append(torch.tanh(x_real + self.G(x_real, c_trg)))\n",
        "\n",
        "                # Save the translated images.\n",
        "                x_concat = torch.cat(x_fake_list, dim=3)\n",
        "                result_path = os.path.join(self.result_dir, '{}-images.jpg'.format(i+1))\n",
        "                save_image(self.denorm(x_concat.data.cpu()), result_path, nrow=1, padding=0)\n",
        "                print('Saved real and fake images into {}...'.format(result_path))\n",
        "\n",
        "\n",
        "\n",
        "    def test_brats(self):\n",
        "        \"\"\"Translate images using Fixed-Point GAN trained on a single dataset.\"\"\"\n",
        "        # Load the trained generator.\n",
        "        self.restore_model(self.test_iters)\n",
        "        \n",
        "        # Set data loader.\n",
        "        if self.dataset in ['BRATS']:\n",
        "            data_loader = self.data_loader\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for i, (x_real, c_org) in enumerate(data_loader):\n",
        "                x_real = x_real.to(self.device)\n",
        "\n",
        "                c_trg = c_org.clone()\n",
        "                c_trg[:, 0] = 0 # always to healthy              \n",
        "                c_trg_list = [c_trg.to(self.device)]\n",
        "\n",
        "                # Translate images.\n",
        "                x_fake_list = [x_real]\n",
        "                for c_trg in c_trg_list:\n",
        "                    delta = self.G(x_real, c_trg)\n",
        "                    delta_org = torch.abs(torch.tanh(delta + x_real) - x_real) - 1.0\n",
        "                    delta_gray = np.mean(delta_org.data.cpu().numpy(), axis=1)\n",
        "                    delta_gray_norm = []\n",
        "\n",
        "                    loc = []\n",
        "                    cls_mul = []\n",
        "\n",
        "                    for indx in range(delta_gray.shape[0]):\n",
        "                        temp = delta_gray[indx, :, :] + 1.0  \n",
        "                        tempimg_th = np.percentile(temp, 99)\n",
        "                        tempimg = np.float32(temp >= tempimg_th)\n",
        "                        temploc = np.reshape(tempimg, (self.image_size*self.image_size, 1))\n",
        "\n",
        "                        kmeans = KMeans(n_clusters=2, random_state=0).fit(temploc)\n",
        "                        labels = kmeans.predict(temploc)\n",
        "\n",
        "                        recreated_loc = self.recreate_image(kmeans.cluster_centers_, labels, self.image_size, self.image_size)\n",
        "                        recreated_loc = ((recreated_loc - np.min(recreated_loc)) / (np.max(recreated_loc) - np.min(recreated_loc)))\n",
        "\n",
        "                        loc.append(recreated_loc)\n",
        "                        delta_gray_norm.append( tempimg )\n",
        "\n",
        "\n",
        "                    loc = np.array(loc, dtype=np.float32)[:, :, :, 0]\n",
        "                    delta_gray_norm = np.array(delta_gray_norm)\n",
        "\n",
        "                    loc = (loc * 2.0) - 1.0\n",
        "                    delta_gray_norm = (delta_gray_norm * 2.0) - 1.0\n",
        "\n",
        "                    x_fake_list.append( torch.from_numpy(np.repeat(delta_gray[:, np.newaxis, :, :], 3, axis=1)).to(self.device) ) # difference map\n",
        "                    x_fake_list.append( torch.from_numpy(np.repeat(delta_gray_norm[:, np.newaxis, :, :], 3, axis=1)).to(self.device) ) # localization thershold\n",
        "                    x_fake_list.append( torch.from_numpy(np.repeat(loc[:, np.newaxis, :, :], 3, axis=1)).to(self.device) ) # localization kmeans\n",
        "                    x_fake_list.append( torch.tanh(delta + x_real) ) # generated image\n",
        "\n",
        "                # Save the translated images.\n",
        "                x_concat = torch.cat(x_fake_list, dim=3)\n",
        "                result_path = os.path.join(self.result_dir, '{}-images.jpg'.format(i+1))\n",
        "                save_image(self.denorm(x_concat.data.cpu()), result_path, nrow=1, padding=0)\n",
        "                print('Saved real and fake images into {}...'.format(result_path))\n",
        "\n",
        "\n",
        "\n",
        "    def cleanup():\n",
        "      dist.destroy_process_group()   \n",
        "\n",
        "# Solver               \n",
        "\n",
        "from torch.utils import data\n",
        "from torchvision import transforms as T\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "import torch\n",
        "import os\n",
        "import random\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "class CelebA(data.Dataset):\n",
        "    \"\"\"Dataset class for the CelebA dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, image_dir, attr_path, selected_attrs, transform, mode):\n",
        "        \"\"\"Initialize and preprocess the CelebA dataset.\"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.attr_path = attr_path\n",
        "        self.selected_attrs = selected_attrs\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        self.train_dataset = []\n",
        "        self.test_dataset = []\n",
        "        self.attr2idx = {}\n",
        "        self.idx2attr = {}\n",
        "        self.preprocess()\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.num_images = len(self.train_dataset)\n",
        "        else:\n",
        "            self.num_images = len(self.test_dataset)\n",
        "\n",
        "    def preprocess(self):\n",
        "        \"\"\"Preprocess the CelebA attribute file.\"\"\"\n",
        "        lines = [line.rstrip() for line in open(self.attr_path, 'r')]\n",
        "        all_attr_names = lines[1].split()\n",
        "        for i, attr_name in enumerate(all_attr_names):\n",
        "            self.attr2idx[attr_name] = i\n",
        "            self.idx2attr[i] = attr_name\n",
        "\n",
        "        lines = lines[2:]\n",
        "        random.seed(1234)\n",
        "        random.shuffle(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            split = line.split()\n",
        "            filename = split[0]\n",
        "            values = split[1:]\n",
        "\n",
        "            label = []\n",
        "            for attr_name in self.selected_attrs:\n",
        "                idx = self.attr2idx[attr_name]\n",
        "                label.append(values[idx] == '1')\n",
        "\n",
        "            if (i+1) < 2000:\n",
        "                self.test_dataset.append([filename, label])\n",
        "            else:\n",
        "                self.train_dataset.append([filename, label])\n",
        "\n",
        "        print('Finished preprocessing the CelebA dataset...')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Return one image and its corresponding attribute label.\"\"\"\n",
        "        dataset = self.train_dataset if self.mode == 'train' else self.test_dataset\n",
        "        filename, label = dataset[index]\n",
        "\n",
        "        # when gcp is used \n",
        "        os.environ['GCLOUD_PROJECT'] = config.GCP_project_id\n",
        "        Bucket_name=config.GCP_Bucket_name\n",
        "        # Use Key authentication\n",
        "        storage_client = storage.Client()\n",
        "        # create bucket\n",
        "        bucket = storage_client.get_bucket(Bucket_name)\n",
        "\n",
        "        # 'data/celeba/images/'\n",
        "        # data_hd/CelebA/\n",
        "        blob = bucket.blob('data_hd/CelebA/'+filename)\n",
        "        filename__=blob.download_as_string()\n",
        "        image=Image.open(BytesIO(filename__))\n",
        "\n",
        "        # when gcp is not used\n",
        "        #image = Image.open(os.path.join(self.image_dir, filename))\n",
        "        return self.transform(image), torch.FloatTensor(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of images.\"\"\"\n",
        "        return self.num_images\n",
        "\n",
        "\n",
        "class BRATS_SYN(data.Dataset):\n",
        "    \"\"\"Dataset class for the BRATS dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, image_dir, transform, mode):\n",
        "        \"\"\"Initialize and Load the BRATS dataset.\"\"\"\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "        self.train_dataset = []\n",
        "        self.test_dataset = []\n",
        "        self.load_data()\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.num_images = len(self.train_dataset)\n",
        "        else:\n",
        "            self.num_images = len(self.test_dataset)\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load BRATS dataset\"\"\"\n",
        "        \n",
        "        # Load test dataset\n",
        "        test_neg = glob(os.path.join(self.image_dir, 'test', 'negative', '*jpg'))\n",
        "        test_pos = glob(os.path.join(self.image_dir, 'test', 'positive', '*jpg'))\n",
        "\n",
        "        for filename in test_neg:\n",
        "            self.test_dataset.append([filename, [0]])\n",
        "\n",
        "        for filename in test_pos:\n",
        "            self.test_dataset.append([filename, [1]])\n",
        "\n",
        "\n",
        "        # Load train dataset\n",
        "        train_neg = glob(os.path.join(self.image_dir, 'train', 'negative', '*jpg'))\n",
        "        train_pos = glob(os.path.join(self.image_dir, 'train', 'positive', '*jpg'))\n",
        "\n",
        "        for filename in train_neg:\n",
        "            self.train_dataset.append([filename, [0]])\n",
        "\n",
        "        for filename in train_pos:\n",
        "            self.train_dataset.append([filename, [1]])\n",
        "\n",
        "        print('Finished loading the BRATS dataset...')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Return one image and its corresponding attribute label.\"\"\"\n",
        "        dataset = self.train_dataset if self.mode == 'train' else self.test_dataset\n",
        "        filename, label = dataset[index]\n",
        "        image = Image.open(filename)\n",
        "        return self.transform(image), torch.FloatTensor(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of images.\"\"\"\n",
        "        return self.num_images\n",
        "\n",
        "\n",
        "def get_loader(image_dir, attr_path, selected_attrs, crop_size=178, image_size=128, \n",
        "               batch_size=16, dataset='CelebA', mode='train', num_workers=1):\n",
        "    \"\"\"Build and return a data loader.\"\"\"\n",
        "    transform = []\n",
        "    if mode == 'train':\n",
        "        transform.append(T.RandomHorizontalFlip())\n",
        "    transform.append(T.CenterCrop(crop_size))\n",
        "    transform.append(T.Resize(image_size))\n",
        "    transform.append(T.ToTensor())\n",
        "    transform.append(T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)))\n",
        "    transform = T.Compose(transform)\n",
        "\n",
        "    if dataset == 'CelebA':\n",
        "        dataset =  CelebA(image_dir, attr_path, selected_attrs, transform, mode)\n",
        "    elif dataset == 'BRATS':\n",
        "        dataset = BRATS_SYN(image_dir, transform, mode)\n",
        "    elif dataset == 'Directory':\n",
        "        dataset = ImageFolder(image_dir, transform)\n",
        "\n",
        "    data_loader = data.DataLoader(dataset=dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=(mode=='train'),\n",
        "                                  num_workers=num_workers)\n",
        "    return data_loader\n",
        "\n",
        "                                \n",
        "                \n",
        "# Main\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "#from solver import Solver\n",
        "#from data_loader import get_loader\n",
        "from torch.backends import cudnn\n",
        "from google.cloud import storage\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from io import BytesIO\n",
        "#from PIL import Image\n",
        "import zipfile\n",
        "from glob import glob\n",
        "from tensorboard import version\n",
        "\n",
        "# detect directory\n",
        "# current directory: path at terminal when executing this file\n",
        "cwd = os.getcwd()\n",
        "print(\"Curret Directory: \",cwd)\n",
        "\n",
        "\n",
        "def str2bool(v):\n",
        "    return v.lower() in ('true')\n",
        "\n",
        "def main(config):\n",
        "    # For fast training.\n",
        "    cudnn.benchmark = True #True\n",
        "    cudnn.enabled = True # added\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "\n",
        "    # Create directories if not exist.\n",
        "    if not os.path.exists(config.log_dir):\n",
        "        os.makedirs(config.log_dir)\n",
        "    print(os.path.exists(config.log_dir))\n",
        "    if not os.path.exists(config.model_save_dir):\n",
        "        os.makedirs(config.model_save_dir)\n",
        "    print(os.path.exists(config.model_save_dir))\n",
        "    if not os.path.exists(config.sample_dir):\n",
        "        os.makedirs(config.sample_dir)\n",
        "    print(os.path.exists(config.sample_dir))\n",
        "    if not os.path.exists(config.result_dir):\n",
        "        os.makedirs(config.result_dir)\n",
        "    print(os.path.exists(config.result_dir))\n",
        "\n",
        "    #os.environ['GCLOUD_PROJECT'] = config.GCP_project_id\n",
        "    Bucket_name=config.GCP_Bucket_name\n",
        "\n",
        "    # Use Key authentication\n",
        "    #storage_client = storage.Client()\n",
        "\n",
        "    # create bucket\n",
        "    #bucket = storage_client.get_bucket(Bucket_name)\n",
        "\n",
        "    # read dataset.zip from bucket and copy it to directory\n",
        "    #t0=time.time()\n",
        "    #for blob in bucket.list_blobs(prefix= config.dataset_in_cloud):\n",
        "    #  print(blob.name)\n",
        "    #  tempfile= cwd + \"/dataset__gcp.zip\"\n",
        "    #  blob.download_to_filename(tempfile)\n",
        "    #t_end_zip=time.time()-t0\n",
        "    #print('dataset gcp moved to local')\n",
        "\n",
        "    #time.sleep(1)\n",
        "\n",
        "    # read from directort zip file and extract\n",
        "    #t0=time.time()\n",
        "    #with zipfile.ZipFile(\"dataset__gcp.zip\",\"r\") as zip_ref:\n",
        "    #    zip_ref.extractall(config.dataset_folder_name)\n",
        "    #t_end_unzip=time.time()-t0\n",
        "    #print('Extraction completed')\n",
        "\n",
        "    #time.sleep(1)\n",
        "\n",
        "\n",
        "        \n",
        "    # Data loader.\n",
        "    data_loader = None\n",
        "\n",
        "    # 'data/celeba/images'\n",
        "    # config.image_dir\n",
        "\n",
        "    if config.dataset in ['CelebA']:#CelebA\n",
        "        data_loader = get_loader(config.image_dir, config.attr_path, config.selected_attrs,\n",
        "                                   config.crop_size, config.image_size, config.batch_size,\n",
        "                                   'CelebA', config.mode, config.num_workers)\n",
        "\n",
        "\n",
        "    elif config.dataset in ['BRATS']:\n",
        "        data_loader = get_loader('{}/brats/syn'.format(config.dataset_folder_name), None, None,\n",
        "                                   config.crop_size, config.image_size, config.batch_size,\n",
        "                                   'BRATS', config.mode, config.num_workers)\n",
        "\n",
        "\n",
        "    elif config.dataset in ['Directory']:\n",
        "        data_loader = get_loader('config.image_dir', None, None,\n",
        "                                 config.crop_size, config.image_size, config.batch_size,\n",
        "                                 'Directory', config.mode, config.num_workers)\n",
        "\n",
        "        \n",
        "    # Solver for training and testing Fixed-Point GAN.\n",
        "    solver = Solver(data_loader, config)\n",
        "\n",
        "    if config.mode == 'train':\n",
        "        if config.dataset in ['CelebA', 'BRATS', 'Directory']:  #CelebA\n",
        "            solver.train()\n",
        "    elif config.mode == 'test':\n",
        "        if config.dataset in ['CelebA', 'Directory']:\n",
        "            solver.test()\n",
        "    elif config.mode == 'test_brats':\n",
        "        if config.dataset in ['BRATS']:\n",
        "            solver.test_brats()\n",
        "\n",
        "    def Copy_local_dir_to_GCP_bucket(directory,folder_name_to_be_copied,gcp_output_folder):\n",
        "        \n",
        "        # to be later on fixed to be controlled from the main\n",
        "        os.environ['GCLOUD_PROJECT'] = config.GCP_project_id\n",
        "        Bucket_name=config.GCP_Bucket_name\n",
        "        # Use Key authentication\n",
        "        storage_client = storage.Client()\n",
        "        # create bucket\n",
        "        bucket = storage_client.get_bucket(Bucket_name)\n",
        "\n",
        "        # Get the list of all files in directory tree at given path\n",
        "        path = directory + \"/\" + folder_name_to_be_copied + \"/\"\n",
        "        #we shall store all the file names in this list\n",
        "        filelist = []\n",
        "\n",
        "        for root, dirs, files in os.walk(path):\n",
        "          for file in files:\n",
        "            #append the file name to the list\n",
        "            filelist.append(os.path.join(root,file))\n",
        "\n",
        "        # write files and subdirectories to gcp\n",
        "        for name in filelist:\n",
        "            # print(name)\n",
        "            blob=bucket.blob(name.replace(directory+\"/\" , gcp_output_folder+\"/\"))\n",
        "            blob.upload_from_filename(name)\n",
        "        return print('Output were transfered to gcp bucket')\n",
        "\n",
        "\n",
        "    if config.dataset == 'BRATS':\n",
        "      t0=time.time()\n",
        "      Copy_local_dir_to_GCP_bucket(cwd,'brats_syn_256_lambda0.1', bucket,\"Output\")\n",
        "      t_end_copy_folder_to_gcp=time.time()-t0\n",
        "      print('time to end copying folders to gcp bucket',t_end_copy_folder_to_gcp)\n",
        "    elif config.dataset == 'CelebA': # CelebA\n",
        "      t0=time.time()\n",
        "      Copy_local_dir_to_GCP_bucket(cwd,'celeba_1',\"Output\")\n",
        "      t_end_copy_folder_to_gcp=time.time()-t0\n",
        "      print('time to end copying folders to gcp bucket',t_end_copy_folder_to_gcp)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Model configuration.\n",
        "    parser.add_argument('--c_dim', type=int, default=5, help='dimension of domain labels (1st dataset)')\n",
        "    parser.add_argument('--c2_dim', type=int, default=8, help='dimension of domain labels (2nd dataset)')\n",
        "    parser.add_argument('--crop_size', type=int, default=1024, help='crop size for the images') # was 178\n",
        "    parser.add_argument('--image_size', type=int, default=128, help='image resolution')\n",
        "    parser.add_argument('--g_conv_dim', type=int, default=16, help='number of conv filters in the first layer of G') # was 64\n",
        "    parser.add_argument('--d_conv_dim', type=int, default=16, help='number of conv filters in the first layer of D') # was 64\n",
        "    parser.add_argument('--g_repeat_num', type=int, default=8, help='number of residual blocks in G') # ws 6\n",
        "    parser.add_argument('--d_repeat_num', type=int, default=8, help='number of strided conv layers in D') # was 6\n",
        "    parser.add_argument('--lambda_cls', type=float, default=1, help='weight for domain classification loss')\n",
        "    parser.add_argument('--lambda_rec', type=float, default=10, help='weight for reconstruction loss')\n",
        "    parser.add_argument('--lambda_gp', type=float, default=10, help='weight for gradient penalty')\n",
        "    parser.add_argument('--lambda_id', type=float, default=10, help='weight for identity loss')\n",
        "    parser.add_argument('--depth', type=int, default=5, help='Number of output images/layers')\n",
        "\n",
        "    # Training configuration.\n",
        "    parser.add_argument('--dataset', type=str, default='CelebA', choices=['CelebA', 'BRATS', 'Directory'])#CelebA\n",
        "    parser.add_argument('--batch_size', type=int, default=16, help='mini-batch size')\n",
        "    parser.add_argument('--num_iters', type=int, default=200000, help='number of total iterations for training D')\n",
        "    parser.add_argument('--num_iters_decay', type=int, default=100000, help='number of iterations for decaying lr')\n",
        "    parser.add_argument('--g_lr', type=float, default=0.0001, help='learning rate for G')\n",
        "    parser.add_argument('--d_lr', type=float, default=0.0001, help='learning rate for D')\n",
        "    parser.add_argument('--n_critic', type=int, default=5, help='number of D updates per each G update')\n",
        "    parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for Adam optimizer')\n",
        "    parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for Adam optimizer')\n",
        "    parser.add_argument('--resume_iters', type=int, default=None, help='resume training from this step')\n",
        "    parser.add_argument('--selected_attrs', '--list', nargs='+', help='selected attributes for the CelebA dataset',\n",
        "                        default=['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young'])\n",
        "\n",
        "    # Test configuration.\n",
        "    parser.add_argument('--test_iters', type=int, default=200000, help='test model from this step')\n",
        "\n",
        "    # Miscellaneous.\n",
        "    parser.add_argument('--num_workers', type=int, default=1)\n",
        "    parser.add_argument('--mode', type=str, default='train', choices=['train', 'test', 'test_brats'])\n",
        "    parser.add_argument('--use_tensorboard', type=str2bool, default=True)\n",
        "    parser.add_argument('--job-dir', help=\" will be passed through from the AI-Platform to your program when you define it.\")\n",
        "    #parser.add_argument('--ngpu', type=int, default=0)\n",
        "    parser.add_argument('--GCP_project_id', type=str, default='fpcityuni-2020')\n",
        "    parser.add_argument('--GCP_Bucket_name', type=str, default=\"fpcityuni-2020-storage\")\n",
        "    \n",
        "    # Directories.\n",
        "    parser.add_argument('--image_dir', type=str, default='data/celeba/images')\n",
        "    \n",
        "    parser.add_argument('--dataset_in_cloud', type=str, default='dataset.zip')\n",
        "    parser.add_argument('--dataset_folder_name', type=str, default='data')\n",
        "    parser.add_argument('--attr_path', type=str, default='data/celeba/list_attr_celeba.txt')\n",
        "    parser.add_argument('--log_dir', type=str, default='celeba/logs')\n",
        "    parser.add_argument('--model_save_dir', type=str, default='celeba/models')\n",
        "    parser.add_argument('--sample_dir', type=str, default='celeba/samples')\n",
        "    parser.add_argument('--result_dir', type=str, default='celeba/results')\n",
        "\n",
        "    # Step size.\n",
        "    parser.add_argument('--log_step', type=int, default=10)\n",
        "    parser.add_argument('--sample_step', type=int, default=1000)\n",
        "    parser.add_argument('--model_save_step', type=int, default=10000)\n",
        "    parser.add_argument('--lr_update_step', type=int, default=1000)\n",
        "\n",
        "    config = parser.parse_args()\n",
        "    print(config, flush='True')\n",
        "    print('__Tensorflow VERSION',tf.__version__)\n",
        "    print('__Tensorboard VERSION:', version.VERSION)\n",
        "    print('__pyTorch VERSION:', torch.__version__)\n",
        "    #import torchvision\n",
        "    #print('__Torchvision VERSION',torchvision.__version__)\n",
        "    print('__CUDA VERSION',torch.version.cuda )\n",
        "    from subprocess import call\n",
        "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
        "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
        "    print('__Devices')\n",
        "    if torch.cuda.device_count() >0:\n",
        "      print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
        "      print('Current device name',torch.cuda.get_device_name(0))\n",
        "    else:\n",
        "      print('No Cuda Device')\n",
        "    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    main(config)\n",
        "    print(torch.cuda.memory_allocated())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting GAN_tf_nGPU_for_GCP_2_test_losses_Method2_scaledimagesaveraging.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUHkIynprpb_",
        "colab_type": "text"
      },
      "source": [
        "# Method 2 Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeCnDd9gH-z6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b31f720-b35f-4304-c840-9ea28f42460a"
      },
      "source": [
        "! python3 -u GAN_tf_nGPU_for_GCP_2_test_losses_Method2_scaledimagesaveraging.py --mode train --dataset 'CelebA' --image_size 1024 --crop_size 1024 --c_dim 5 --g_conv_dim 16 --d_conv_dim 16 --depth 5 \\\n",
        "                 --GCP_project_id fpcityuni-2020 \\\n",
        "                 --GCP_Bucket_name fpcityuni-2020-storage \\\n",
        "                 --image_dir data/test_max_size_of_readable_images \\\n",
        "                 --attr_path list_attr_celeba_hd.txt \\\n",
        "                 --sample_dir celeba_1/samples \\\n",
        "                 --log_dir celeba_1/logs \\\n",
        "                 --model_save_dir celeba_1/models \\\n",
        "                 --result_dir celeba_1/results \\\n",
        "                 --selected_attrs Black_Hair Blond_Hair Brown_Hair Male Young --lambda_id 10 \\\n",
        "                 --batch_size 3 --num_workers 3 --num_iters 50000 --log_step 50 --sample_step 5000 #--resume_iters 10000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-08-15 21:21:11.301196: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Curret Directory:  /content/drive/My Drive/FinalProject-GCP\n",
            "Namespace(GCP_Bucket_name='fpcityuni-2020-storage', GCP_project_id='fpcityuni-2020', attr_path='list_attr_celeba_hd.txt', batch_size=3, beta1=0.5, beta2=0.999, c2_dim=8, c_dim=5, crop_size=1024, d_conv_dim=16, d_lr=0.0001, d_repeat_num=8, dataset='CelebA', dataset_folder_name='data', dataset_in_cloud='dataset.zip', depth=5, g_conv_dim=16, g_lr=0.0001, g_repeat_num=8, image_dir='data/test_max_size_of_readable_images', image_size=1024, job_dir=None, lambda_cls=1, lambda_gp=10, lambda_id=10.0, lambda_rec=10, log_dir='celeba_1/logs', log_step=50, lr_update_step=1000, mode='train', model_save_dir='celeba_1/models', model_save_step=10000, n_critic=5, num_iters=50000, num_iters_decay=100000, num_workers=3, result_dir='celeba_1/results', resume_iters=None, sample_dir='celeba_1/samples', sample_step=5000, selected_attrs=['Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Male', 'Young'], test_iters=200000, use_tensorboard=True)\n",
            "__Tensorflow VERSION 2.3.0\n",
            "__Tensorboard VERSION: 2.3.0\n",
            "__pyTorch VERSION: 1.6.0+cu101\n",
            "__CUDA VERSION 10.1\n",
            "__CUDNN VERSION: 7603\n",
            "__Number CUDA Devices: 1\n",
            "__Devices\n",
            "Active CUDA Device: GPU 0\n",
            "Current device name Tesla P100-PCIE-16GB\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "Finished preprocessing the CelebA dataset...\n",
            "Number of Cuda devices:  1\n",
            "kernel size 4\n",
            "Generator(\n",
            "  (main_DownSampling_bottleneck): Sequential(\n",
            "    (0): Conv2d(8, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
            "    (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (14): ReLU(inplace=True)\n",
            "    (15): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (16): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): ResidualBlock(\n",
            "      (main): Sequential(\n",
            "        (0): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (19): ResidualBlock(\n",
            "      (main): Sequential(\n",
            "        (0): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (20): ResidualBlock(\n",
            "      (main): Sequential(\n",
            "        (0): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (21): ResidualBlock(\n",
            "      (main): Sequential(\n",
            "        (0): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (22): ResidualBlock(\n",
            "      (main): Sequential(\n",
            "        (0): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (23): ResidualBlock(\n",
            "      (main): Sequential(\n",
            "        (0): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (24): ResidualBlock(\n",
            "      (main): Sequential(\n",
            "        (0): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (25): ResidualBlock(\n",
            "      (main): Sequential(\n",
            "        (0): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): depthwise_separable_conv(\n",
            "          (depthwise): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
            "          (pointwise): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        )\n",
            "        (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (UpsamplingLayers): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (rgb_converters): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(16, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Generator\n",
            "The number of parameters: 9882143\n",
            "Discriminator(\n",
            "  (rgb_to_features): ModuleList(\n",
            "    (0): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (2): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (3): Conv2d(3, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (4): Conv2d(3, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (Dis_blocks): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Conv2d(35, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (3): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): Conv2d(67, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "      (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (3): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): Conv2d(131, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "      (2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (3): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): Conv2d(259, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "      (2): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (3): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): LeakyReLU(negative_slope=0.01)\n",
            "    )\n",
            "  )\n",
            "  (conv1): Conv2d(2048, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(2048, 5, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (3): LeakyReLU(negative_slope=0.01)\n",
            "    (4): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (5): LeakyReLU(negative_slope=0.01)\n",
            "  )\n",
            ")\n",
            "Discriminator\n",
            "The number of parameters: 45717538\n",
            "2020-08-15 21:21:18.559503: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-08-15 21:21:18.559856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:21:18.560780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-08-15 21:21:18.560815: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-15 21:21:18.562813: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-15 21:21:18.564541: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-15 21:21:18.564925: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-15 21:21:18.566541: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-15 21:21:18.567352: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-15 21:21:18.571437: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-15 21:21:18.571607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:21:18.572569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:21:18.573467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-15 21:21:18.579531: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz\n",
            "2020-08-15 21:21:18.580386: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xa76d1340 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-15 21:21:18.580431: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-08-15 21:21:18.581927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:21:18.583247: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xa76d1500 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-08-15 21:21:18.583281: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-08-15 21:21:18.583490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:21:18.584416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2020-08-15 21:21:18.584456: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-15 21:21:18.584513: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-08-15 21:21:18.584555: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-08-15 21:21:18.584608: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-08-15 21:21:18.584685: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-08-15 21:21:18.584732: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-08-15 21:21:18.584775: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-08-15 21:21:18.584918: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:21:18.585845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:21:18.586666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-08-15 21:21:18.586719: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-08-15 21:21:18.586790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-08-15 21:21:18.586813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-08-15 21:21:18.586832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-08-15 21:21:18.587048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:21:18.587992: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-08-15 21:21:18.588809: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-08-15 21:21:18.588855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14265 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "Start training...\n",
            "Elapsed [0:01:26], Iteration [50/50000], D/loss_real: 231.1274, D/loss_fake: -356.1314, D/loss_cls: 3.3252, D/loss_gp: 1.1543, d_loss: -110.1363, G/loss_fake: 544.5530, G/loss_rec: 0.0954, G/loss_cls: 3.2818, G/loss_fake_id: 546.1896, G/loss_rec_id: 0.0956, G/loss_cls_id: 3.2469, G/loss_id: 0.0588, g_loss: 1099.7690\n",
            "Elapsed [0:02:45], Iteration [100/50000], D/loss_real: -908.4854, D/loss_fake: 818.1950, D/loss_cls: 3.4994, D/loss_gp: 1.7083, d_loss: -69.7077, G/loss_fake: -891.2453, G/loss_rec: 0.0674, G/loss_cls: 3.4904, G/loss_fake_id: -891.2453, G/loss_rec_id: 0.0674, G/loss_cls_id: 3.4904, G/loss_id: 0.0390, g_loss: -1773.7710\n",
            "Traceback (most recent call last):\n",
            "  File \"GAN_tf_nGPU_for_GCP_2_test_losses_Method2_scaledimagesaveraging.py\", line 1649, in <module>\n",
            "    main(config)\n",
            "  File \"GAN_tf_nGPU_for_GCP_2_test_losses_Method2_scaledimagesaveraging.py\", line 1521, in main\n",
            "    solver.train()\n",
            "  File \"GAN_tf_nGPU_for_GCP_2_test_losses_Method2_scaledimagesaveraging.py\", line 919, in train\n",
            "    d_loss.backward()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 185, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 127, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}